# Comparing `tmp/pyquokka-0.2.7-py3-none-any.whl.zip` & `tmp/pyquokka-0.2.8-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,36 +1,41 @@
-Zip file size: 275662 bytes, number of entries: 34
--rw-rw-r--  2.0 unx      256 b- defN 23-Apr-15 21:15 pyquokka/__init__.py
--rw-rw-r--  2.0 unx     4291 b- defN 23-Apr-19 05:39 pyquokka/catalog.py
--rw-rw-r--  2.0 unx      372 b- defN 23-Apr-12 19:11 pyquokka/common_startup.sh
--rw-rw-r--  2.0 unx    28239 b- defN 23-Apr-16 23:17 pyquokka/coordinator.py
--rw-rw-r--  2.0 unx    48229 b- defN 23-Apr-16 23:13 pyquokka/core.py
--rw-rw-r--  2.0 unx    48426 b- defN 23-Apr-19 05:39 pyquokka/dataset.py
--rw-rw-r--  2.0 unx    96482 b- defN 23-Apr-19 18:58 pyquokka/datastream.py
--rw-rw-r--  2.0 unx     1438 b- defN 22-Nov-11 18:24 pyquokka/debugger.py
--rw-rw-r--  2.0 unx    75930 b- defN 23-Apr-16 23:19 pyquokka/df.py
--rw-rw-r--  2.0 unx    37360 b- defN 23-Apr-16 23:19 pyquokka/executors.py
--rw-rw-r--  2.0 unx    12657 b- defN 23-Mar-28 22:05 pyquokka/expression.py
--rw-rw-r--  2.0 unx    17483 b- defN 23-Mar-26 21:54 pyquokka/flight.py
--rw-rw-r--  2.0 unx     3108 b- defN 23-Mar-20 21:38 pyquokka/hbq.py
--rwxrwxr-x  2.0 unx   368480 b- defN 23-Apr-11 05:32 pyquokka/ldb.so
--rw-rw-r--  2.0 unx      274 b- defN 23-Mar-25 18:43 pyquokka/leader_start_ray.sh
--rw-rw-r--  2.0 unx      619 b- defN 23-Mar-25 18:43 pyquokka/leader_startup.sh
--rw-rw-r--  2.0 unx    27823 b- defN 23-Apr-13 23:50 pyquokka/logical.py
--rw-rw-r--  2.0 unx     4314 b- defN 23-Apr-15 21:27 pyquokka/orderedstream.py
--rw-rw-r--  2.0 unx      886 b- defN 23-Mar-15 22:44 pyquokka/placement_strategy.py
--rw-rw-r--  2.0 unx     3717 b- defN 23-Mar-29 00:48 pyquokka/quokka_dataset.py
--rw-rw-r--  2.0 unx    19755 b- defN 23-Apr-14 04:33 pyquokka/quokka_runtime.py
--rw-rw-r--  2.0 unx    93718 b- defN 22-Oct-04 03:50 pyquokka/redis.conf
--rw-rw-r--  2.0 unx    16471 b- defN 23-Mar-27 22:47 pyquokka/sql_utils.py
--rw-rw-r--  2.0 unx     2371 b- defN 22-Jul-15 20:59 pyquokka/state.py
--rw-rw-r--  2.0 unx    11695 b- defN 23-Apr-09 03:05 pyquokka/tables.py
--rw-rw-r--  2.0 unx     2351 b- defN 23-Jan-28 04:17 pyquokka/target_info.py
--rw-rw-r--  2.0 unx     5752 b- defN 23-Mar-20 21:30 pyquokka/task.py
--rw-rw-r--  2.0 unx    29799 b- defN 23-Apr-21 00:29 pyquokka/utils.py
--rw-rw-r--  2.0 unx     4081 b- defN 23-Jan-25 17:17 pyquokka/windowtypes.py
--rw-rw-r--  2.0 unx    11357 b- defN 23-Apr-21 00:31 pyquokka-0.2.7.dist-info/LICENSE
--rw-rw-r--  2.0 unx     1040 b- defN 23-Apr-21 00:31 pyquokka-0.2.7.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-21 00:31 pyquokka-0.2.7.dist-info/WHEEL
--rw-rw-r--  2.0 unx        9 b- defN 23-Apr-21 00:31 pyquokka-0.2.7.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2659 b- defN 23-Apr-21 00:31 pyquokka-0.2.7.dist-info/RECORD
-34 files, 981534 bytes uncompressed, 271520 bytes compressed:  72.3%
+Zip file size: 288013 bytes, number of entries: 39
+-rw-rw-r--  2.0 unx      309 b- defN 23-May-09 16:09 pyquokka/__init__.py
+-rw-rw-r--  2.0 unx     4291 b- defN 23-Apr-21 22:04 pyquokka/catalog.py
+-rw-rw-r--  2.0 unx      372 b- defN 23-Apr-21 22:04 pyquokka/common_startup.sh
+-rw-rw-r--  2.0 unx    28239 b- defN 23-Apr-21 22:04 pyquokka/coordinator.py
+-rw-rw-r--  2.0 unx    48179 b- defN 23-May-09 05:45 pyquokka/core.py
+-rw-rw-r--  2.0 unx    48426 b- defN 23-Apr-21 22:04 pyquokka/dataset.py
+-rw-rw-r--  2.0 unx    98944 b- defN 23-May-09 05:06 pyquokka/datastream.py
+-rw-rw-r--  2.0 unx     1438 b- defN 23-Apr-21 22:04 pyquokka/debugger.py
+-rw-rw-r--  2.0 unx    76129 b- defN 23-May-09 15:51 pyquokka/df.py
+-rw-rw-r--  2.0 unx    37360 b- defN 23-Apr-21 22:04 pyquokka/executors.py
+-rw-rw-r--  2.0 unx    12657 b- defN 23-Apr-21 22:04 pyquokka/expression.py
+-rw-rw-r--  2.0 unx    17483 b- defN 23-Apr-21 22:04 pyquokka/flight.py
+-rw-rw-r--  2.0 unx     3098 b- defN 23-May-09 03:48 pyquokka/hbq.py
+-rwxrwxr-x  2.0 unx   368480 b- defN 23-Apr-21 22:04 pyquokka/ldb.so
+-rw-rw-r--  2.0 unx      274 b- defN 23-Apr-21 22:04 pyquokka/leader_start_ray.sh
+-rw-rw-r--  2.0 unx      619 b- defN 23-Apr-21 22:04 pyquokka/leader_startup.sh
+-rw-rw-r--  2.0 unx    29383 b- defN 23-May-09 05:30 pyquokka/logical.py
+-rw-rw-r--  2.0 unx     9100 b- defN 23-Apr-21 22:04 pyquokka/orderedstream.py
+-rw-rw-r--  2.0 unx      886 b- defN 23-Apr-21 22:04 pyquokka/placement_strategy.py
+-rw-rw-r--  2.0 unx     3717 b- defN 23-Apr-21 22:04 pyquokka/quokka_dataset.py
+-rw-rw-r--  2.0 unx    19735 b- defN 23-May-09 03:48 pyquokka/quokka_runtime.py
+-rw-rw-r--  2.0 unx    93718 b- defN 23-Apr-21 22:04 pyquokka/redis.conf
+-rw-rw-r--  2.0 unx    16471 b- defN 23-Apr-21 22:04 pyquokka/sql_utils.py
+-rw-rw-r--  2.0 unx     2371 b- defN 23-Apr-21 22:04 pyquokka/state.py
+-rw-rw-r--  2.0 unx    11695 b- defN 23-Apr-21 22:04 pyquokka/tables.py
+-rw-rw-r--  2.0 unx     2351 b- defN 23-Apr-21 22:04 pyquokka/target_info.py
+-rw-rw-r--  2.0 unx     5752 b- defN 23-Apr-21 22:04 pyquokka/task.py
+-rw-rw-r--  2.0 unx    39151 b- defN 23-May-09 15:31 pyquokka/utils.py
+-rw-rw-r--  2.0 unx     4081 b- defN 23-Apr-21 22:04 pyquokka/windowtypes.py
+-rw-rw-r--  2.0 unx      146 b- defN 23-May-09 04:03 pyquokka/executors/__init__.py
+-rw-rw-r--  2.0 unx      811 b- defN 23-May-08 15:34 pyquokka/executors/base_executor.py
+-rw-rw-r--  2.0 unx    17511 b- defN 23-May-09 04:03 pyquokka/executors/sql_executors.py
+-rw-rw-r--  2.0 unx    17185 b- defN 23-May-09 04:03 pyquokka/executors/ts_executors.py
+-rw-rw-r--  2.0 unx     2870 b- defN 23-May-09 05:34 pyquokka/executors/vector_executors.py
+-rw-rw-r--  2.0 unx    11357 b- defN 23-May-09 16:15 pyquokka-0.2.8.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     1020 b- defN 23-May-09 16:15 pyquokka-0.2.8.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-09 16:15 pyquokka-0.2.8.dist-info/WHEEL
+-rwxrwxr-x  2.0 unx        9 b- defN 23-May-09 16:15 pyquokka-0.2.8.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3116 b- defN 23-May-09 16:15 pyquokka-0.2.8.dist-info/RECORD
+39 files, 1038826 bytes uncompressed, 283147 bytes compressed:  72.7%
```

## zipnote {}

```diff
@@ -81,23 +81,38 @@
 
 Filename: pyquokka/utils.py
 Comment: 
 
 Filename: pyquokka/windowtypes.py
 Comment: 
 
-Filename: pyquokka-0.2.7.dist-info/LICENSE
+Filename: pyquokka/executors/__init__.py
 Comment: 
 
-Filename: pyquokka-0.2.7.dist-info/METADATA
+Filename: pyquokka/executors/base_executor.py
 Comment: 
 
-Filename: pyquokka-0.2.7.dist-info/WHEEL
+Filename: pyquokka/executors/sql_executors.py
 Comment: 
 
-Filename: pyquokka-0.2.7.dist-info/top_level.txt
+Filename: pyquokka/executors/ts_executors.py
 Comment: 
 
-Filename: pyquokka-0.2.7.dist-info/RECORD
+Filename: pyquokka/executors/vector_executors.py
+Comment: 
+
+Filename: pyquokka-0.2.8.dist-info/LICENSE
+Comment: 
+
+Filename: pyquokka-0.2.8.dist-info/METADATA
+Comment: 
+
+Filename: pyquokka-0.2.8.dist-info/WHEEL
+Comment: 
+
+Filename: pyquokka-0.2.8.dist-info/top_level.txt
+Comment: 
+
+Filename: pyquokka-0.2.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pyquokka/__init__.py

```diff
@@ -6,7 +6,10 @@
 from . import utils
 from . import datastream
 from . import orderedstream
 from . import tables
 from . import task
 from . import hbq
 
+from .df import QuokkaContext
+
+__version__ = '0.2.8'
```

## pyquokka/core.py

```diff
@@ -152,18 +152,18 @@
         def partition_fn(predicate_fn, partitioner_fn, batch_funcs, projection, num_target_channels, x, source_channel):
 
             start = time.time()
             # x could be either a pyarrow table of a polars dataframe
             # print(predicate_fn)
             if type(predicate_fn) == str:
                 con = duckdb.connect().execute('PRAGMA threads=%d' % 8)
-                batch_arrow = x.to_arrow() if type(x) == polars.internals.DataFrame else x
+                batch_arrow = x.to_arrow() if type(x) == polars.DataFrame else x
                 x = polars.from_arrow(con.execute(predicate_fn).arrow())
             else:
-                x = x if type(x) == polars.internals.DataFrame else polars.from_arrow(x)
+                x = x if type(x) == polars.DataFrame else polars.from_arrow(x)
                 x = x.filter(predicate_fn)
             # print("filter time", time.time() - start)
 
             for func in batch_funcs:
                 x = func(x)
                 if x is None or len(x) == 0:
                     return {}
@@ -277,15 +277,15 @@
         
         partition_fns = self.partition_fns[source_actor_id]
 
         start_convert = time.time()
         if output is None:
             assert from_local
         else:
-            assert type(output) == pyarrow.Table or type(output) == polars.internals.DataFrame, "push data type {} not understood".format(type(output))
+            assert type(output) == pyarrow.Table or type(output) == polars.DataFrame, "push data type {} not understood".format(type(output))
 
         print_if_profile("convert time", time.time() - start_convert)
 
         for target_actor_id in partition_fns:
 
             if target_mask is not None and target_actor_id not in target_mask:
                 continue
@@ -446,16 +446,16 @@
             pass
 
     def process_output(self, actor_id, channel_id, output, transaction, state_seq, out_seq):
 
         if output is not None:
             if type(output) == pyarrow.Table:
                 output = polars.from_arrow(output)
-            assert type(output) == polars.internals.DataFrame or type(output) == types.GeneratorType
-            if type(output) == polars.internals.DataFrame:
+            assert type(output) == polars.DataFrame or type(output) == types.GeneratorType
+            if type(output) == polars.DataFrame:
                 output = [output]
             
             for data in output:
                 if actor_id not in self.blocking_nodes:
                     pushed = self.push(actor_id, channel_id, out_seq, data)
                     # after a while we realized this output is actually none.
```

## pyquokka/datastream.py

```diff
@@ -5,14 +5,15 @@
 from pyquokka.quokka_runtime import *
 from pyquokka.expression import * 
 from pyquokka.utils import EC2Cluster, LocalCluster
 from pyquokka.sql_utils import required_columns_from_exp, label_sample_table_names
 from functools import partial
 import pyarrow as pa
 from sqlglot.dataframe.sql import functions as F
+from threadpoolctl import threadpool_limits
 import numpy as np
 
 class DataStream:
 
     """
     Quokka DataStream class is how most users are expected to interact with Quokka.
     However users are not expected to create a DataStream directly by calling its constructor.
@@ -388,21 +389,71 @@
             transformed = self.transform(f, new_schema = self.schema, required_columns=self.schema)
             return transformed
         else:
             return self.quokka_context.new_stream(sources={0: self}, partitioners={0: PassThroughPartitioner()}, node=FilterNode(self.schema, predicate),
                                               schema=self.schema, sorted = self.sorted)
 
 
-    def filter_ann(self, vec_column, query_vectors, k):
+    def nn_probe(self, probe_df, vec_column = None, vec_column_left = None, vec_column_right = None, k = 1, suffix = "_probe"):
 
         """
-        This will filter the DataStream where the vec_column is a 
+        This will perform a nearest neighbor join between two Quokka
         
         """
 
+        if vec_column is not None:
+            assert vec_column_left is None and vec_column_right is None, "cannot specify both vec_column and vec_column_left/vec_column_right"
+            vec_column_left = vec_column
+            vec_column_right = vec_column
+        else:
+            assert vec_column_right is not None and vec_column_left is not None, "must specify either vec_column or vec_column_left/vec_column_right"
+        
+        assert k >= 1, "k must be at least 1"
+        assert type(probe_df) == polars.DataFrame, "probe_df must be a polars DataFrame"
+
+        assert vec_column_left in self.schema, "Vector column not in schema"
+        assert vec_column_right in probe_df.schema, "Probe column not in schema"
+
+        # rename any column in probe_df that is also in self.schema
+
+        probe_vec_col = vec_column_right
+        new_schema = self.schema
+        schema_mapping = {col: {0: col} for col in self.schema}
+        for col_name in probe_df.columns:                
+            if col_name in self.schema:
+                assert col_name + suffix not in probe_df.columns, "suffix not sufficient to avoid column name collision"
+                probe_df = probe_df.rename({col_name: col_name + suffix})
+                if col_name == vec_column_right:
+                    probe_vec_col = col_name + suffix
+                new_schema.append(col_name + suffix)
+                schema_mapping[col_name + suffix] = {-1: col_name}
+            else:
+                new_schema.append(col_name)
+                schema_mapping[col_name] = {-1: col_name}
+
+        node = NearestNeighborFilterNode(new_schema, schema_mapping, vec_column, probe_df, probe_vec_col,  k)
+
+        return self.quokka_context.new_stream(sources={0: self}, partitioners={0: PassThroughPartitioner()}, node=node,
+                                              schema=new_schema, sorted = self.sorted)
+
+    def ann_join(self, vec_column = None, vec_column_left = None, vec_column_right = None, probe_side = "left", k = 1):
+
+        """
+        This will perform a nearest neighbor join between two Quokka DataStreams.
+        The plan is to convert this to a NearestNeighborFilterNode after you propagate cardinality and figure out which side is smaller.
+        
+        """
+
+        pass # we need to eventually compile this down to some kind of filter
+        # node = NearestNeighborFilterNode(new_schema, vec_column, query_vectors,  k, probe_vector_col)
+
+        # return self.quokka_context.new_stream(sources={0: self}, partitioners={0: PassThroughPartitioner()}, node=node,
+        #                                       schema=new_schema, sorted = self.sorted)
+
+
     def select(self, columns: list):
 
         """
         This will create a new DataStream that contains only selected columns from the source DataStream.
 
         Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each
         batch directly results from selecting columns from a batch in the source DataStream. While this certainly may be the case, `select()` is aggressively 
@@ -957,15 +1008,14 @@
             >>> d = d.gramian(["l_quantity", "l_extendedprice"])
 
             Result will be a 2x2 matrix.
         
         """
 
         def udf2(x):
-            from threadpoolctl import threadpool_limits
             x = x.select(columns).to_numpy() - demean
             with threadpool_limits(limits=8, user_api='blas'):
                 product = np.dot(x.transpose(), x)
             return polars.from_numpy(product, schema = columns)
 
         for col in columns:
             assert col in self.schema
@@ -1361,28 +1411,28 @@
         assert issubclass(type(right), DataStream), "must join against a Quokka DataStream"
 
         if maintain_sort_order is not None:
 
             assert how in {"inner", "left"}
 
             # our broadcast join strategy should automatically satisfy this, no need to do anything special
-            if type(right) == polars.internals.DataFrame:
+            if type(right) == polars.DataFrame:
                 assert maintain_sort_order == "left"
                 assert self.sorted is not None
             
             else:
                 assert maintain_sort_order in {"left", "right"}
                 if maintain_sort_order == "left":
                     assert self.sorted is not None
                 else:
                     assert right.sorted is not None
                 if how == "left":
                     assert maintain_sort_order == "right", "in a left join, can only maintain order of the right table"
         
-        #if type(right) == polars.internals.DataFrame and right.to_arrow().nbytes > 10485760:
+        #if type(right) == polars.DataFrame and right.to_arrow().nbytes > 10485760:
         #    raise Exception("You cannot join a DataStream against a Polars DataFrame more than 10MB in size. Sorry.")
 
         if on is None:
             assert left_on is not None and right_on is not None
             assert left_on in self.schema, "join key not found in left table"
             assert right_on in right.schema, "join key not found in right table"
         else:
@@ -1979,15 +2029,15 @@
         self.orderby = orderby
     
     def cogroup(self, right, executor: Executor, new_schema: list, required_cols_left = None, required_cols_right = None):
         """
         Purely Experimental API.
         """
 
-        assert (type(right) == GroupedDataStream or type(right) == polars.internals.DataFrame) and issubclass(type(executor), Executor)
+        assert (type(right) == GroupedDataStream) and issubclass(type(executor), Executor)
         
         assert len(self.groupby) == 1 and len(right.groupby) == 1, "we only support single key partition functions right now"
         assert self.groupby[0] == right.groupby[0], "must be grouped by the same key"
         copartitioner = self.groupby[0]
 
         schema_mapping={col: {-1: col} for col in new_schema}
 
@@ -2001,31 +2051,26 @@
         if required_cols_right is None:
             required_cols_right = set(right.source_data_stream.schema)
         else:
             if type(required_cols_right) == list:
                 required_cols_right = set(required_cols_right)
             assert type(required_cols_right) == set
 
-        if type(right) == GroupedDataStream:
 
-            return self.source_data_stream.quokka_context.new_stream(
-                sources={0: self.source_data_stream, 1: right.source_data_stream},
-                partitioners={0: HashPartitioner(
-                    copartitioner), 1: HashPartitioner(copartitioner)},
-                node=StatefulNode(
-                    schema=new_schema,
-                    schema_mapping=schema_mapping,
-                    required_columns={0: required_cols_left, 1: required_cols_right},
-                    operator= executor),
+        return self.source_data_stream.quokka_context.new_stream(
+            sources={0: self.source_data_stream, 1: right.source_data_stream},
+            partitioners={0: HashPartitioner(
+                copartitioner), 1: HashPartitioner(copartitioner)},
+            node=StatefulNode(
                 schema=new_schema,
-                )
-
-        elif type(right) == polars.internals.DataFrame:
-            
-            raise NotImplementedError
+                schema_mapping=schema_mapping,
+                required_columns={0: required_cols_left, 1: required_cols_right},
+                operator= executor),
+            schema=new_schema,
+            )
 
     def count_distinct(self, col: str):
 
         """
         Count the number of distinct values of a column for each group. This may result in out of memory. This is not approximate.
 
         Args:
```

## pyquokka/df.py

```diff
@@ -1,13 +1,14 @@
 import graphviz
 import copy
 import polars
 import pyarrow.csv as csv
 import pyquokka.sql_utils as sql_utils
 from pyquokka.datastream import * 
+from pyquokka.orderedstream import * 
 from pyquokka.catalog import *
 import pyarrow.parquet as pq
 from pyarrow.fs import S3FileSystem
 import os
 
 class QuokkaContext:
     def __init__(self, cluster = None, io_per_node = 2, exec_per_node = 1) -> None:
@@ -64,15 +65,17 @@
         self.exec_config = {"hbq_path": "/data/", "fault_tolerance": False, "memory_limit": 0.25, "max_pipeline_batches": 30, 
                         "checkpoint_interval": None, "checkpoint_bucket": "quokka-checkpoint", "batch_attempt": 20, "max_pipeline": 3}
 
         self.latest_node_id = 0
         self.nodes = {}
         self.cluster = LocalCluster() if cluster is None else cluster
         if type(self.cluster) == LocalCluster:
-            self.exec_config["fault_tolerance"] = False, "Fault tolerance is not supported in local mode, turning it off"
+            if self.exec_config["fault_tolerance"]:
+                print("Fault tolerance is not supported in local mode, turning it off")
+                self.exec_config["fault_tolerance"] = False
         self.io_per_node = io_per_node
         self.exec_per_node = exec_per_node
 
         self.coordinator = Coordinator.options(num_cpus=0.001, max_concurrency = 2,resources={"node:" + str(self.cluster.leader_private_ip): 0.001}).remote()
         self.catalog = Catalog.options(num_cpus=0.001,resources={"node:" + str(self.cluster.leader_private_ip): 0.001}).remote()
         self.dataset_manager = ArrowDataset.options(num_cpus = 0.001, resources={"node:" + str(self.cluster.leader_private_ip): 0.001}).remote()
 
@@ -761,19 +764,19 @@
                                  0: PassThroughPartitioner()}, node=DataSetNode(schema), schema=schema)
         return DataSet(self, schema, stream.source_node_id)
 
     def optimize(self, node_id):
         self.__push_ann__()
         self.__push_filter__(node_id)
         self.__early_projection__(node_id)
-        self.__fold_map__(node_id)
-        if self.sql_config["optimize_joins"]:
-            self.__merge_joins__(node_id)
-        self.__propagate_cardinality__(node_id)
-        self.__determine_stages__(node_id)
+        # self.__fold_map__(node_id)
+        # if self.sql_config["optimize_joins"]:
+        #     self.__merge_joins__(node_id)
+        # self.__propagate_cardinality__(node_id)
+        # self.__determine_stages__(node_id)
         
         assert len(self.execution_nodes[node_id].parents) == 1
         parent_idx = list(self.execution_nodes[node_id].parents)[0]
         parent_id = self.execution_nodes[node_id].parents[parent_idx]
 
         if issubclass(type(self.execution_nodes[parent_id]), SourceNode):
             self.execution_nodes[node_id].blocking = True
@@ -1186,42 +1189,43 @@
                 del parent.targets[node_id]
                 del self.execution_nodes[node_id]
 
             # otherwise you need to maintain yourself
             else:
 
                 # first remove yourself from the current location in the graph, i.e. wire up your parent to your targets.
-                parent = self.execution_nodes[node.parents[0]]
-                for target_id in targets:
-                    parent.targets[target_id] = targets[target_id]
-                del parent.targets[node_id]
-                # set your targets parents to your parent
-                for target_id in targets:
-                    success = False
-                    for key in self.execution_nodes[target_id].parents:
-                        if self.execution_nodes[target_id].parents[key] == node_id:
-                            self.execution_nodes[target_id].parents[key] = node.parents[0]
-                            success = True
-                            break
-                    assert success
-                node.targets = {}
+                # parent = self.execution_nodes[node.parents[0]]
+                # for target_id in targets:
+                #     parent.targets[target_id] = targets[target_id]
+                # del parent.targets[node_id]
+                # # set your targets parents to your parent
+                # for target_id in targets:
+                #     success = False
+                #     for key in self.execution_nodes[target_id].parents:
+                #         if self.execution_nodes[target_id].parents[key] == node_id:
+                #             self.execution_nodes[target_id].parents[key] = node.parents[0]
+                #             success = True
+                #             break
+                #     assert success
+                # node.targets = {}
                 
-                # now insert yourself between curr_node_id and curr_target_id
+                # # now insert yourself between curr_node_id and curr_target_id
 
-                self.execution_nodes[curr_node_id].targets[node_id] = TargetInfo(PassThroughPartitioner(), None, None, [])
-                node.parents[0] = curr_node_id
-                node.targets[curr_target_id] = copy.deepcopy(self.execution_nodes[curr_node_id].targets[curr_target_id])
-                del self.execution_nodes[curr_node_id].targets[curr_target_id]
-                success = False
-                for key in self.execution_nodes[curr_target_id].parents:
-                    if self.execution_nodes[target_id].parents[key] == curr_node_id:
-                        self.execution_nodes[target_id].parents[key] = node_id
-                        success = True
-                        break
-                assert success
+                # self.execution_nodes[curr_node_id].targets[node_id] = TargetInfo(PassThroughPartitioner(), None, None, [])
+                # node.parents[0] = curr_node_id
+                # node.targets[curr_target_id] = copy.deepcopy(self.execution_nodes[curr_node_id].targets[curr_target_id])
+                # del self.execution_nodes[curr_node_id].targets[curr_target_id]
+                # success = False
+                # for key in self.execution_nodes[curr_target_id].parents:
+                #     if self.execution_nodes[target_id].parents[key] == curr_node_id:
+                #         self.execution_nodes[target_id].parents[key] = node_id
+                #         success = True
+                #         break
+                # assert success
+                pass
 
     def __fold_map__(self, node_id):
 
         node = self.execution_nodes[node_id]
         targets = node.targets
 
         if issubclass(type(node), SourceNode):
```

## pyquokka/hbq.py

```diff
@@ -52,15 +52,15 @@
             table.write_ipc(where)
             return True
 
         assert type(outputs) == dict
         new_outputs = {}
         # futures = []
         for key in outputs:
-            assert type(outputs[key]) == polars.internals.DataFrame
+            assert type(outputs[key]) == polars.DataFrame
             new_outputs[key] = self.path + "hbq-" + str(source_actor_id) + "-" + str(source_channel_id) + "-" + str(seq) \
                 + "-" + str(target_actor_id) + "-" + str(key) + ".ipc"
             outputs[key].write_ipc( new_outputs[key] )
             # futures.append(self.executor.submit(put_ipc, outputs[key], new_outputs[key]))
 
         # assert all([fut.result() for fut in futures])
         self.store[source_actor_id, source_channel_id, seq, target_actor_id] = new_outputs
```

## pyquokka/logical.py

```diff
@@ -571,31 +571,54 @@
     
     def lower(self, task_graph, parent_nodes, parent_source_info):
         print("Tried to lower a filter node. This means the optimization probably failed and something bad is happening.")
         raise NotImplementedError
 
 class NearestNeighborFilterNode(TaskNode):
 
-    def __init__(self, schema: list, vec_column: str, query_vectors,  k: int) -> None:
+    def __init__(self, schema: list, schema_mapping: dict, vec_column: str, probe_df, probe_vector_col: str, k: int) -> None:
         super().__init__(
             schema = schema,
-            schema_mapping = {column: {0: column} for column in schema},
+            schema_mapping = schema_mapping,
             required_columns = {0: {vec_column}})
         self.vec_column = vec_column
-        self.query_vectors = query_vectors
+        self.probe_df = probe_df
         self.k = k
+        self.probe_vector_col = probe_vector_col
     
     def __str__(self):
-        result = "nearest neighbor filter node on vec column {} with {} query vectors".format(self.vec_column, len(self.query_vectors))
+        result = "nearest neighbor filter node on vec column {} with {} probe vectors".format(self.vec_column, len(self.probe_df))
         return result
     
+    def set_cardinality(self, parent_cardinalities):
+        assert len(parent_cardinalities) == 1
+        parent_cardinality = list(parent_cardinalities.values())[0]
+        for target in self.targets:
+            if self.targets[target].predicate == sqlglot.exp.TRUE and parent_cardinality is not None:
+                self.cardinality[target] = parent_cardinality * self.k
+            else:
+                self.cardinality[target] = None
+    
     def lower(self, task_graph, parent_nodes, parent_source_info):
-        print("Tried to lower a nearest neighbor filter node. This means the optimization probably failed and something bad is happening.")
-        raise NotImplementedError
 
+        executor1 = DFProbeDataStreamNNExecutor1(self.vec_column, self.probe_df, self.probe_vector_col, self.k)
+        executor2 = DFProbeDataStreamNNExecutor2(self.vec_column, self.probe_df, self.probe_vector_col, self.k)
+
+        intermediate_target_info = TargetInfo(BroadcastPartitioner(), sqlglot.exp.TRUE, None, [])
+
+        if self.blocking:
+            assert len(self.targets) == 1
+            target_info = self.targets[list(self.targets.keys())[0]]
+            transform_func = target_info_to_transform_func(target_info)     
+            intermediates = task_graph.new_non_blocking_node(parent_nodes,executor1, self.stage, self.placement_strategy, source_target_info=parent_source_info)
+            return task_graph.new_blocking_node({0: intermediates}, executor2, self.stage, SingleChannelStrategy(), source_target_info={0: intermediate_target_info}, transform_fn = transform_func)
+
+        else:
+            intermediates = task_graph.new_non_blocking_node(parent_nodes,executor1, self.stage, self.placement_strategy, source_target_info=parent_source_info)
+            return task_graph.new_non_blocking_node({0: intermediates}, executor2, self.stage, SingleChannelStrategy(), source_target_info={0: intermediate_target_info})
 
 class ProjectionNode(TaskNode):
 
     # predicate must be CNF
     def __init__(self, projection: set) -> None:
         super().__init__(
             schema = projection,
```

## pyquokka/orderedstream.py

```diff
@@ -29,15 +29,119 @@
                 schema_mapping={col: {0: col} for col in self.schema},
                 required_columns={0: set(self.schema)},
                 operator=ShiftOperator(n, fill_value),
                 assume_sorted={0:True},
             ),
             schema=self.schema,
         )
+    
+    def pattern_recognize(self, anchors, duration_limits, partition_by = None):
 
+        """
+        experimental API.
+        """
+
+        query = "select "
+        curr_alias = 0
+        for anchor in anchors:
+            assert type(anchor) == Expression or type(anchor) == str
+            if type(anchor) == Expression:
+                anchor = anchor.sql()
+            columns = set(i.name for i in anchor.find_all(
+            sqlglot.expressions.Column))
+            for column in columns:
+                assert column in self.schema, "Tried to define an anchor using a column not in the schema {}".format(column)
+            query += anchor + " as __anchor_{}, ".format(curr_alias)
+            curr_alias += 1
+            assert "__anchor_{}".format(curr_alias) not in self.schema, "Column called __anchor_{} already exists in the schema".format(curr_alias)
+        query = query[:-2] + " from batch_arrow"
+
+        for duration_limit in duration_limits:
+            assert(type(duration_limit) == int or type(duration_limit) == float)
+
+        duration_buffer = sum(duration_limits)
+        assert len(duration_limits) == len(anchors) - 1
+
+        class CEPExecutor(Executor):
+            def __init__(self) -> None:
+                import ldbpy
+                self.state = None
+                self.cep = ldbpy.CEP(duration_limits)
+                self.con = duckdb.connect().execute('PRAGMA threads=%d' % 8)
+                self.num_anchors = len(anchors)
+
+            def execute(self,batches,stream_id, executor_id):
+                from pyarrow.cffi import ffi
+                os.environ["OMP_NUM_THREADS"] = "8"
+         
+                arrow_batch = pa.concat_tables(batches)
+                # you can only process up to the duration_buffer, the rest needs to be cached
+                if self.state is not None:
+                    arrow_batch = pa.concat_tables([self.state, arrow_batch])
+                
+                if len(arrow_batch) > duration_buffer:
+                    self.state = arrow_batch[-duration_buffer:]
+                    arrow_batch = arrow_batch[: -duration_buffer]
+                else:
+                    self.state = arrow_batch
+                    return
+            
+                result = polars.from_arrow(self.con.execute(query).arrow())
+                array_ptrs = []
+                schema_ptrs = []
+                c_schemas = []
+                c_arrays = []
+                list_of_arrs = []
+                for anchor in range(self.num_anchors):
+                    index = result.select(polars.arg_where(polars.col("__anchor_{}".format(anchor))))
+                    list_of_arrs.append(index.to_arrow()["__anchor_{}".format(anchor)].combine_chunks())
+                    c_schema = ffi.new("struct ArrowSchema*")
+                    c_array = ffi.new("struct ArrowArray*")
+                    c_schemas.append(c_schema)
+                    c_arrays.append(c_array)
+                    schema_ptr = int(ffi.cast("uintptr_t", c_schema))
+                    array_ptr = int(ffi.cast("uintptr_t", c_array))
+                    list_of_arrs[-1]._export_to_c(array_ptr, schema_ptr)
+                    array_ptrs.append(array_ptr)
+                    schema_ptrs.append(schema_ptr)
+
+                result = self.cep.do_arrow_batch(array_ptrs, schema_ptrs)
+                del c_schemas
+                del c_arrays
+                # print("TIME", time.time() - start)
+                
+            def done(self,executor_id):
+                from pyarrow.cffi import ffi
+                if self.state is None:
+                    return 
+
+                arrow_batch = self.state
+                self.state = None
+                result = polars.from_arrow(self.con.execute(query).arrow())
+                array_ptrs = []
+                schema_ptrs = []
+                c_schemas = []
+                c_arrays = []
+                list_of_arrs = []
+                for anchor in range(self.num_anchors):
+                    index = result.select(polars.arg_where(polars.col("__anchor_{}".format(anchor))))
+                    list_of_arrs.append(index.to_arrow()["__anchor_{}".format(anchor)].combine_chunks())
+                    c_schema = ffi.new("struct ArrowSchema*")
+                    c_array = ffi.new("struct ArrowArray*")
+                    c_schemas.append(c_schema)
+                    c_arrays.append(c_array)
+                    schema_ptr = int(ffi.cast("uintptr_t", c_schema))
+                    array_ptr = int(ffi.cast("uintptr_t", c_array))
+                    list_of_arrs[-1]._export_to_c(array_ptr, schema_ptr)
+                    array_ptrs.append(array_ptr)
+                    schema_ptrs.append(schema_ptr)
+
+                result = self.cep.do_arrow_batch(array_ptrs, schema_ptrs)
+
+        
 
     def join_asof(self, right, on=None, left_on=None, right_on=None, by=None, left_by = None, right_by = None, suffix="_2"):
 
         assert type(right) == OrderedStream
         if on is not None:
             assert left_on is None and right_on is None
             left_on = on
```

## pyquokka/quokka_runtime.py

```diff
@@ -209,15 +209,15 @@
             return partial(partition_key_1, target_total_channels // source_total_channels)
 
     def prologue(self, streams, placement_strategy, source_target_info):
 
         def partition_key_str(key, data, source_channel, num_target_channels):
 
             result = {}
-            assert type(data) == polars.internals.DataFrame
+            assert type(data) == polars.DataFrame
             if "int" in str(data[key].dtype).lower():
                 partitions = data.with_columns(polars.Series(name="__partition__", values=(data[key] % num_target_channels))).partition_by("__partition__")
             elif data[key].dtype == polars.datatypes.Utf8 or data[key].dtype == polars.datatypes.Float32 or data[key].dtype == polars.datatypes.Float64:
                 partitions = data.with_columns(polars.Series(name="__partition__", values=(data[key].hash() % num_target_channels))).partition_by("__partition__")
             else:
                 print(data[key])
                 raise Exception("partition key type not supported")
@@ -227,15 +227,15 @@
             return result
         
 
         def partition_key_range(key, total_range, data, source_channel, num_target_channels):
 
             per_channel_range = total_range // num_target_channels
             result = {}
-            assert type(data) == polars.internals.DataFrame
+            assert type(data) == polars.DataFrame
             partitions = data.with_columns(polars.Series(name="__partition__", values=((data[key] - 1) // per_channel_range))).partition_by("__partition__")
             for partition in partitions:
                 target = partition["__partition__"][0]
                 result[target] = partition.drop("__partition__")   
             return result 
 
         def broadcast(data, source_channel, num_target_channels):
```

## pyquokka/utils.py

```diff
@@ -5,15 +5,17 @@
 import multiprocessing
 import pyquokka
 import ray
 import json
 import signal
 import polars
 import multiprocessing
-from pssh.clients import ParallelSSHClient
+import concurrent.futures
+import yaml
+import subprocess
 
 def preexec_function():
     # Ignore the SIGINT signal by setting the handler to the standard
     # signal handler SIG_IGN.
     signal.signal(signal.SIGINT, signal.SIG_IGN)
 
 class EC2Cluster:
@@ -140,14 +142,16 @@
     return os.system("ssh -oStrictHostKeyChecking=no -i {} ubuntu@{} 'bash -s' < {}".format(key_location, x, pyquokka.__file__.replace("__init__.py", "common_startup.sh")))
 
 def get_cluster_from_docker_head(spill_dir = "/data"):
     import socket
     self_ip = socket.gethostbyname(socket.gethostname())
     ray.init("ray://" + self_ip + ":10001")
     private_ips = [name.split(":")[1] for name in ray.cluster_resources().keys() if "node" in name]
+    # rotate private_ips so that self_ip is the first element
+    private_ips = private_ips[private_ips.index(self_ip):] + private_ips[:private_ips.index(self_ip)]
     cpu_count = ray.cluster_resources()["CPU"] // len(private_ips)
     return EC2Cluster([None] * len(private_ips), private_ips, [None] * len(private_ips), cpu_count, spill_dir, docker_head_private_ip=self_ip)
 
 class QuokkaClusterManager:
 
     def __init__(self, key_name = None, key_location = None, security_group= None) -> None:
         
@@ -186,31 +190,30 @@
         return {int(i):d[i] for i in d}
     
     def install_python_package(self, cluster, req):
         assert type(cluster) == EC2Cluster
         self.launch_all("pip3 install " + req, list(cluster.public_ips.values()), "Failed to install " + req)
 
     def launch_ssh_command(self, command, ip, ignore_error=False):
-        launch_command = "ssh -oStrictHostKeyChecking=no -oConnectTimeout=2 -i " + self.key_location + " ubuntu@" + ip + " " + command
+        launch_command = "ssh -oStrictHostKeyChecking=no -oConnectTimeout=5 -i " + self.key_location + " ubuntu@" + ip + " " + command
         try:
             result = subprocess.run(launch_command, shell=True, capture_output=True, check=True)
             return result.stdout.decode().strip()
         except subprocess.CalledProcessError as e:
             raise Exception(f"launch_ssh_command failed with exit code: {e.returncode}")
-
+        
     def launch_all(self, command, ips, error = "Error", ignore_error = False):
-        client = ParallelSSHClient(ips, user="ubuntu", pkey=self.key_location, timeout=5)
-        output = client.run_command(command)
-        result = []
-        for host_output in output:
-            for line in host_output.stdout:
-                result.append(line)
-            exit_code = host_output.exit_code
-            assert exit_code == 0 or ignore_error, exit_code
-        return result
+
+        with concurrent.futures.ThreadPoolExecutor(max_workers=len(ips)) as executor:
+            future_list = [executor.submit(self.launch_ssh_command, command, instance_ip) for instance_ip in ips]
+
+        results = [future.result() for future in future_list]
+
+        return results
+
 
     def copy_all(self, file_path, ips, error = "Error"):
         commands = ["scp -oStrictHostKeyChecking=no -oConnectTimeout=2 -i " + self.key_location + " " + file_path + " ubuntu@" + str(ip) + ":. " for ip in ips]
         processes = [subprocess.Popen(command, close_fds=True, shell=True) for command in commands]
         return_codes = [process.wait() for process in processes]
         if sum(return_codes) != 0:
             raise Exception(error)
@@ -255,46 +258,55 @@
             
         pool = multiprocessing.Pool(multiprocessing.cpu_count())        
         pool.starmap(execute_script, [(self.key_location, public_ip) for public_ip in public_ips])
 
         self.launch_all("aws configure set aws_secret_access_key " + str(aws_access_key), public_ips, "Failed to set AWS access key")
         self.launch_all("aws configure set aws_access_key_id " + str(aws_access_id), public_ips, "Failed to set AWS access id")
 
+        print("----Completed aws access key setting----")
         # cluster must have same ray version as client.
         requirements = ["ray==" + ray.__version__, "polars==" + polars.__version__,  "pyquokka"] + requirements
         for req in requirements:
             assert type(req) == str
             try:
                 self.launch_all("pip3 install " + req, public_ips, "Failed to install " + req)
             except:
                 pass
+        
+        print("----Finished setting up envs----")
 
     def copy_and_launch_flight(self, public_ips):
         
         self.copy_all(pyquokka.__file__.replace("__init__.py","flight.py"), public_ips, "Failed to copy flight server file.")
         self.launch_all("export GLIBC_TUNABLES=glibc.malloc.trim_threshold=524288", public_ips, "Failed to set malloc limit")
         self.launch_all("nohup python3 -u flight.py > foo.out 2> foo.err < /dev/null &", public_ips, "Failed to start flight servers on workers.")
 
     def set_up_spill_dir(self, public_ips, spill_dir):
+
         print("Trying to set up spill dir.")   
         result = self.launch_all("sudo nvme list", public_ips, "failed to list nvme devices")
-        devices = [sentence.split(" ")[0] for sentence in result if "Amazon EC2 NVMe Instance Storage" in sentence]
+        devices = []
+
+        for sentence in result:
+            if "Amazon EC2 NVMe Instance Storage" in sentence:
+                split_by_newline = sentence.split("\n")
+                device = split_by_newline[2].split(" ")[0]
+                devices.append(device)
+
         if len(devices) == 0:
             print("No nvme devices found. Skipping.")
             return
+
         assert all([device == devices[0] for device in devices]), "All instances must have same nvme device location. Raise Github issue if you see this."
         device = devices[0]
         print("Found nvme device: ", device)
-        
-        try:
-            self.launch_all("sudo mkfs.ext4 -F -E nodiscard {};".format(device), public_ips, "failed to format nvme ssd")
-            self.launch_all("sudo mount {} {};".format(device, spill_dir), public_ips, "failed to mount nvme ssd")
-            self.launch_all("sudo chmod -R a+rw {}".format(spill_dir), public_ips, "failed to give spill dir permissions")
-        except:
-            pass
+        self.launch_all("sudo mkfs.ext4 -F -E nodiscard {};".format(device), public_ips, "failed to format nvme ssd")
+        self.launch_all("sudo mount {} {};".format(device, spill_dir), public_ips, "failed to mount nvme ssd")
+        self.launch_all("sudo chmod -R a+rw {}".format(spill_dir), public_ips, "failed to give spill dir permissions")
+
 
     def create_cluster(self, aws_access_key, aws_access_id, num_instances, instance_type = "i3.2xlarge", ami="ami-0530ca8899fac469f", requirements = [], spill_dir = "/data"):
 
         """
         Create a Ray cluster configured to run Quokka applications.
 
         Args:
@@ -469,15 +481,15 @@
             private_ips = private_ips[leader_index:] + private_ips[:leader_index]
 
             return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
         else:
             print("Cluster in an inconsistent state. Either only some machines are running or some machines have been terminated.")
             return False
     
-    def get_cluster_from_ray(self, path_to_yaml, aws_access_key, aws_access_id, requirements = [], spill_dir = "/data", cluster_name = None):
+    def get_cluster_from_ray(self, path_to_yaml, aws_access_key, aws_access_id, requirements = [], spill_dir = "/data"):
 
         """
         Connect to a Ray cluster. This will set up the Quokka runtime on the cluster. The Ray cluster must be in a running state and created by 
         the `ray up` command. The `ray up` command creates a yaml file that is used to connect to the cluster. This function will read the yaml file
         and connect to the cluster.
 
         Make sure all the instances are running before calling this function! Best wait for a few minutes after calling `ray up` before calling this function.
@@ -507,22 +519,22 @@
 
             >>> cluster.to_json("my_cluster.json")
             >>> cluster = manager.get_cluster_from_json("my_cluster.json")
         
         """
 
         import yaml
-        ec2 = boto3.client("ec2")
         with open(path_to_yaml, 'r') as f:
             config = yaml.safe_load(f)
         
+        region = config["provider"]["region"]
+        ec2 = boto3.client("ec2", region_name=region)
     
         tag_key = "ray-cluster-name"
-        if cluster_name is None:
-            cluster_name = config['cluster_name']
+        cluster_name = config['cluster_name']
         instance_type = config["available_node_types"]['ray.worker.default']["node_config"]["InstanceType"]
         cpu_count = ec2.describe_instance_types(InstanceTypes=[instance_type])['InstanceTypes'][0]['VCpuInfo']['DefaultVCpus']
 
         filters = [{'Name': 'instance-state-name', 'Values': ['running']},
                    {'Name': f'tag:{tag_key}', 'Values': [cluster_name]}]
         response = ec2.describe_instances(Filters=filters)
         instance_ids = []
@@ -552,17 +564,197 @@
         self.set_up_envs(public_ips, requirements, aws_access_key, aws_access_id)
         self.launch_all("sudo mkdir {}".format(spill_dir), public_ips, "failed to make temp spill directory", ignore_error = True)
         self.set_up_spill_dir(public_ips, spill_dir)
 
         z = os.system("ssh -oStrictHostKeyChecking=no -i " + self.key_location + " ubuntu@" + public_ips[0] + " 'bash -s' < " + pyquokka.__file__.replace("__init__.py","leader_startup.sh"))
         print(z)
 
+        print(f"-----Before launching redis-----")
         self.copy_and_launch_flight(public_ips)
+        print(f"-----Returning EC2 cluster------")
         return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
 
+    
+    def get_multiple_clusters_from_yaml(self, paths_to_yaml, aws_access_key, aws_access_id, requirements = [], spill_dir = "/data"):
+        
+        """
+        Connect to a multiregional Ray cluster. This will set up the Quokka runtime on the cluster. The Ray clusters must be in a running state and created by 
+        the `ray up` command. The `ray up` command creates a yaml file that is used to connect to the cluster. This function will read the provided files
+        and connect all running instances into one cluster. It is important that all instances launched are in the same VPC. To see how to do this,
+        see the tutorial in the Quokka repository.
+
+        Make sure all the instances are running before calling this function! Best wait for a few minutes after calling `ray up` before calling this function.
+
+        Args:
+            paths_to_yaml (list): Paths to the yaml file used by `ray up`.
+            aws_access_key (str): AWS access key.
+            aws_access_id (str): AWS access id.
+            requirements (list): List of python packages to install on the cluster.
+            spill_dir (str): Directory to use for spill files. This is the directory where the Quokka runtime will write spill files.
+                Quokka will detect if your instance have NVME SSD and mount it to this directory.
+        
+        Return:
+            (EC2Cluster: Cluster object, Region_Info: Dictionary)
+
+        Examples:
+
+            You have `us_west_2.yaml`. You call `ray up us-west-2.yaml`. You can then connect to the cluster **after all the instances are running** by doing:
+
+            >>> from pyquokka.utils import *
+            >>> manager = QuokkaClusterManager()
+            >>> cluster_list = ["my_cluster_us.yaml", "my_cluster_eu.yaml"]
+            >>> results = manager.get_multiple_clusters_from_yaml(cluster_list, aws_access_key, aws_access_id, requirements = ["numpy", "pandas"], spill_dir = "/data")
+            >>> from pyquokka.df import QuokkaContext
+            >>> qc = QuokkaContext(results[0])
+        """
+
+
+        def get_instance_info_from_cluster(path_to_yaml):
+
+            """
+            Gets instances information of the specified cluster.
+
+            Args:
+                path_to_yaml (str): Path to the yaml file used by `ray up`.
+            
+            Return:
+                (instance_ids, public_ips, private_ips, vcpu_per_node, region)
+            """
+            
+            
+            with open(path_to_yaml, 'r') as f:
+                config = yaml.safe_load(f)
+            
+            region = config["provider"]["region"]
+            ec2 = boto3.client("ec2", region_name=region)
+        
+            tag_key = "ray-cluster-name"
+            cluster_name = config['cluster_name']
+            instance_type = config["available_node_types"]['ray.worker.default']["node_config"]["InstanceType"]
+            cpu_count = ec2.describe_instance_types(InstanceTypes=[instance_type])['InstanceTypes'][0]['VCpuInfo']['DefaultVCpus']
+
+            filters = [{'Name': 'instance-state-name', 'Values': ['running']},
+                    {'Name': f'tag:{tag_key}', 'Values': [cluster_name]}]
+            response = ec2.describe_instances(Filters=filters)
+            instance_ids = []
+            public_ips = []
+            private_ips = []
+
+            instance_names = [[k for k in instance['Tags'] if k['Key'] == 'ray-user-node-type'][0]['Value'] for reservation in response['Reservations'] for instance in reservation['Instances']]
+            instance_ids = [instance['InstanceId'] for reservation in response['Reservations'] for instance in reservation['Instances']]
+            public_ips = [instance['PublicIpAddress'] for reservation in response['Reservations'] for instance in reservation['Instances']]
+            private_ips = [instance['PrivateIpAddress'] for reservation in response['Reservations'] for instance in reservation['Instances']]
+
+            try:
+                head_index = instance_names.index("ray.head.default")
+            except:
+                print("No head node found. Please make sure that the cluster is running.")
+                return False
+        
+            # rotate instance_ids, public_ips, private_ips so that head is first
+            instance_ids = instance_ids[head_index:] + instance_ids[:head_index]
+            public_ips = public_ips[head_index:] + public_ips[:head_index]
+            private_ips = private_ips[head_index:] + private_ips[:head_index]
+
+            assert len(instance_ids) == len(public_ips) == len(private_ips)
+            print("Detected {} instances in running ray cluster {} in region ".format(len(instance_ids), cluster_name, region))
+
+            print(public_ips)
+
+            return (instance_ids, public_ips, private_ips, cpu_count, region)
+        
+
+        def get_cluster_from_ips(instance_ids, public_ips, private_ips, cpu_count, aws_access_id, aws_access_key, requirements, spill_dir):
+            
+            self.set_up_envs(public_ips, requirements, aws_access_key, aws_access_id)
+            self.launch_all("sudo mkdir {}".format(spill_dir), public_ips, "failed to make temp spill directory", ignore_error = True)
+            self.set_up_spill_dir(public_ips, spill_dir)
+
+            print(f"----Launching ray cluster----")
+
+            leader_public_ip = public_ips[0]
+            leader_private_ip = private_ips[0]
+
+            z = os.system("ssh -oStrictHostKeyChecking=no -i " + self.key_location + " ubuntu@" + str(leader_public_ip) + " 'bash -s' < " + pyquokka.__file__.replace("__init__.py","leader_startup.sh"))
+            print(z)
+
+            z = os.system("ssh -oStrictHostKeyChecking=no -i " + self.key_location + " ubuntu@" + str(leader_public_ip) + " /home/ubuntu/.local/bin/ray start --disable-usage-stats --head --port=6380")
+            print(z)
+
+            command ="/home/ubuntu/.local/bin/ray start --address='" + str(leader_private_ip) + ":6380' --redis-password='5241590000000000'"
+            # double launches head note (Note: potential issue)
+            self.launch_all(command, public_ips, "ray workers failed to connect to ray head node")
+
+            print(f"-----Before launching flight-----")
+            self.copy_and_launch_flight(public_ips)
+            print(f"-----Returning EC2 cluster------")
+            return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
+
+
+        print("Launching Multi-Region Quokka cluster.")
+
+        all_instance_ids = []
+        all_public_ips = []
+        all_private_ips = []
+        cpu_count = None
+
+        # Dictionary storing infos about all regions in the cluster
+        region_info = {}
+
+        # Get cluster information
+        for path in paths_to_yaml:
+            subcluster_info = get_instance_info_from_cluster(path)
+            
+            # Extend region info dict
+            if subcluster_info[-1] not in region_info:
+                region_info[subcluster_info[-1]] = {}
+            
+            if "instance_ids" not in region_info[subcluster_info[-1]]:
+                region_info[subcluster_info[-1]]["instance_ids"] = []
+            region_info[subcluster_info[-1]]["instance_ids"].extend(subcluster_info[0])
+
+            if "public_ips" not in region_info[subcluster_info[-1]]:
+                region_info[subcluster_info[-1]]["public_ips"] = []
+            region_info[subcluster_info[-1]]["public_ips"].extend(subcluster_info[1])
+
+            if "private_ips" not in region_info[subcluster_info[-1]]:
+                region_info[subcluster_info[-1]]["private_ips"] = []
+            region_info[subcluster_info[-1]]["private_ips"].extend(subcluster_info[2])
+
+            if "vcpu_per_node" not in region_info[subcluster_info[-1]]:
+                region_info[subcluster_info[-1]]["vcpu_per_node"] = []
+            region_info[subcluster_info[-1]]["vcpu_per_node"] = subcluster_info[3]
+
+
+            all_instance_ids.extend(subcluster_info[0])
+            all_public_ips.extend(subcluster_info[1])
+            all_private_ips.extend(subcluster_info[2])
+
+            # Note: Take lowest cpu count
+            if cpu_count == None or subcluster_info[3] < cpu_count:
+                cpu_count = subcluster_info[3]
+
+        print("----Found the following region information----")
+        print(region_info)
+        print("----------------------------------------------")
+
+        print("----Found the following public IPs----")
+        print(all_public_ips)
+        print("--------------------------------------")
+
+        # Stop ray on all cluster instances
+        stop_command = "/home/ubuntu/.local/bin/ray stop --force"
+        print("----Stopping ray on all instances----")
+        self.launch_all(stop_command, all_public_ips)
+
+        # Created cluster across all instances
+        cluster = get_cluster_from_ips(all_instance_ids, all_public_ips, all_private_ips, cpu_count, aws_access_id, aws_access_key, requirements, spill_dir)
+
+        return (cluster, region_info)
+      
     def get_cluster_from_docker_cluster(self, path_to_yaml, spill_dir = "/data", cluster_name = None):
 
         """
         """
 
         import yaml
         ec2 = boto3.client("ec2")
@@ -595,8 +787,8 @@
         public_ips = public_ips[head_index:] + public_ips[:head_index]
         private_ips = private_ips[head_index:] + private_ips[:head_index]
 
         assert len(instance_ids) == len(public_ips) == len(private_ips)
         print("Detected {} instances in running ray cluster {}".format(len(instance_ids), cluster_name))
 
         print(public_ips)
-        return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
+        return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
```

## Comparing `pyquokka-0.2.7.dist-info/LICENSE` & `pyquokka-0.2.8.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pyquokka-0.2.7.dist-info/METADATA` & `pyquokka-0.2.8.dist-info/METADATA`

 * *Files 9% similar despite different names*

```diff
@@ -1,16 +1,15 @@
 Metadata-Version: 2.1
 Name: pyquokka
-Version: 0.2.7
+Version: 0.2.8
 Summary: Quokka
 Author: Tony Wang
 Author-email: zihengw@stanford.edu
 License: http://www.apache.org/licenses/LICENSE-2.0
 Keywords: python
-Platform: UNKNOWN
 Classifier: Development Status :: 3 - Alpha
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: MacOS :: MacOS X
 Classifier: Operating System :: POSIX :: Linux
 License-File: LICENSE
 Requires-Dist: cffi
 Requires-Dist: pyarrow
@@ -36,9 +35,7 @@
 
 Dope way to do cloud analytics
 
 Check out https://github.com/marsupialtail/quokka
 
 or https://marsupialtail.github.io/quokka/
 
-
-
```

## Comparing `pyquokka-0.2.7.dist-info/RECORD` & `pyquokka-0.2.8.dist-info/RECORD`

 * *Files 18% similar despite different names*

```diff
@@ -1,34 +1,39 @@
-pyquokka/__init__.py,sha256=vifW-I2hoQKvSNKPyT0BAX1iVntZ7erS3xXhjq4eH3k,256
+pyquokka/__init__.py,sha256=dCpFpXk81q0aL2Umoye0o4Ad4wsDzYnmvXmavLcJGeU,309
 pyquokka/catalog.py,sha256=XczH9nRAjYE0eWvPdoI_sQE1N85vflPouzjpIe0lqOA,4291
 pyquokka/common_startup.sh,sha256=Rst9lyiO_Fx34swl6_-Z8w7lRHQM-8GDhsUFCbS_kP8,372
 pyquokka/coordinator.py,sha256=s7yjNB_F6bEoWoko_5cTfN0BAyH9AcYm2PKIbqJexhI,28239
-pyquokka/core.py,sha256=wGUxg3M0bIUzBoVbQbAWg-vg3sFhpj4auBVF-cKr784,48229
+pyquokka/core.py,sha256=BQoZLOBXUpI2peE8rIgp1a0gC-Pk0nMCzlz8V1dkq6s,48179
 pyquokka/dataset.py,sha256=vgKT07HkV_P0LVu5JHijEHvYUkeQZElbgLnMC8yw_jg,48426
-pyquokka/datastream.py,sha256=thsNg6wKI5BYTbxD3OksCws_Kszfv69uGhhc32Q91lA,96482
+pyquokka/datastream.py,sha256=X34J074YncZqikKloV4wHGdpTB5kGoBv6TluPSVv1nY,98944
 pyquokka/debugger.py,sha256=Yi1CqGHbV2y2bszUhKuxcQ561vugc6hs6xfnpJ8HIjU,1438
-pyquokka/df.py,sha256=iU3bNgO8PPcDW78jkGDTcZv2zog8De16fWw-VVLecd0,75930
+pyquokka/df.py,sha256=ZbHcGUdHeD2ZvnTspqhfBCvB6G7Yi63ALW1u5qixZEw,76129
 pyquokka/executors.py,sha256=dVzy3_WbsMtbAWay2UlEgE1GmxEVbT6Fz1X9i2elboY,37360
 pyquokka/expression.py,sha256=SmJxvGrSgW3ra8EU5SLGbQ8AxbJ2bPxKHLvbXpQ-8WY,12657
 pyquokka/flight.py,sha256=ri1aEXtn4s2_ACD8PfU5EIy_fA7FlNrM61WuUjQ9Pl8,17483
-pyquokka/hbq.py,sha256=tF679UdNRkjXDdEvBXv6_aXd4dh4nQgIUHRjknKX1LQ,3108
+pyquokka/hbq.py,sha256=V3nE2IcIIclxxdDyXRdzuV7_y_DRa90B_aVaa6aU9tg,3098
 pyquokka/ldb.so,sha256=SO70uhXN219Ns5RNIEePv1fs0qRSksc6yq2eN0tucw8,368480
 pyquokka/leader_start_ray.sh,sha256=vyZi-Utmj7r8Qr43YmOQq3uIuAN5Nn4F61MIszzdit4,274
 pyquokka/leader_startup.sh,sha256=yOP5vjuLS9D2WtcozcFewXQB84p8jm8IcyltYcuWNss,619
-pyquokka/logical.py,sha256=u9sua5Pwt-_xm_QGnQ_0dx7Wf0sVXWOyB3mAKaIE0Yg,27823
-pyquokka/orderedstream.py,sha256=_ADqgW0GxSffcXCs0Z1TDpdwGtxgDDwxzOCtoPr14lQ,4314
+pyquokka/logical.py,sha256=pSxywcLUYY60w8XMZEa7PgOokGpl8vQW_yij6hZVohE,29383
+pyquokka/orderedstream.py,sha256=ciirQMgndCbWOurDGtaZBD0TRF8txdnNB0cIad_wxrc,9100
 pyquokka/placement_strategy.py,sha256=KX1hEDHCTBUfUoQ64Zv5e7iVjTKRh4oAFFs03HIxDqQ,886
 pyquokka/quokka_dataset.py,sha256=_b7L8zWCimtiR3kMbCZWLH4g-w3SZ-J8gw8bjqhv-lA,3717
-pyquokka/quokka_runtime.py,sha256=ADyM7i5KiJxERReBQdesm_kuYxGwPWccG5RiK4ywMK0,19755
+pyquokka/quokka_runtime.py,sha256=LG8kBwWlR20AkVZBHyRcP280qwKr_NAB-TFLyIvinv0,19735
 pyquokka/redis.conf,sha256=Hk0GU-BnDDpMZ6Gmit1-Ct_iyO7ttCvzyfz5PLFVkJY,93718
 pyquokka/sql_utils.py,sha256=1vWMrQft37so5KtOLlsqLk8MpaFB0XO9Ib9XwQEuw90,16471
 pyquokka/state.py,sha256=wGt5uh_ZS-xV-HqVgWmdTZUnQoabpvSOHiQXOejg7L4,2371
 pyquokka/tables.py,sha256=58vsFPBKOEKZ-7Ei7GbhXKsAgw6JguwdfQhayRJrjEI,11695
 pyquokka/target_info.py,sha256=RVldOYWGgxv9YXMSN3J3S-UbSEJxeUMoYY1LcHT3edg,2351
 pyquokka/task.py,sha256=_O_itxDxzXnjojys8___CKSaApTnEpUxVrAvWaU-AVI,5752
-pyquokka/utils.py,sha256=cECTzW_EKCcOja-BAWRP-4XBzrPGGZUhMqdekcSUwW4,29799
+pyquokka/utils.py,sha256=sn_Rs0BIGfkmtN8rNfWGOSKJ_1btLTDk8-SW9SazJQ0,39151
 pyquokka/windowtypes.py,sha256=zPh9QgwSQ3E00eNQM8jk2iQZNMuzAQ3eoaFDm1H5NGk,4081
-pyquokka-0.2.7.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-pyquokka-0.2.7.dist-info/METADATA,sha256=4ijqKpojmvB4u2VZ4QxX0rtogwDFoCj2nAa3Kxz-H1k,1040
-pyquokka-0.2.7.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-pyquokka-0.2.7.dist-info/top_level.txt,sha256=u5sX_ng3imCHha6-wOUpEO0V2TufF_OHADKxb38hwHg,9
-pyquokka-0.2.7.dist-info/RECORD,,
+pyquokka/executors/__init__.py,sha256=48lCF53HMa4Od5yyQfV8T2X0aLL5q9LZ0XVO_0-_MtA,146
+pyquokka/executors/base_executor.py,sha256=clVkP0wf03OMG7x9vMdzRPTviT5DE6rQv3n6E81bZM4,811
+pyquokka/executors/sql_executors.py,sha256=35xL26SqKQQCwqmCVyWxHKtdkHpWCbGoSjvoufJqHFk,17511
+pyquokka/executors/ts_executors.py,sha256=KPRjeMap_PogESYfqYpCBrLyVgRKVrGxWQYICQRehEo,17185
+pyquokka/executors/vector_executors.py,sha256=hs7mrwyXf3Dcvn0pvG9p_IFaJgX0xbCfxUaFj7-WCGQ,2870
+pyquokka-0.2.8.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+pyquokka-0.2.8.dist-info/METADATA,sha256=TFvrGARo30jYPMHNo6hvmcAVBWDlzez4OV6FyENhEME,1020
+pyquokka-0.2.8.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+pyquokka-0.2.8.dist-info/top_level.txt,sha256=u5sX_ng3imCHha6-wOUpEO0V2TufF_OHADKxb38hwHg,9
+pyquokka-0.2.8.dist-info/RECORD,,
```

