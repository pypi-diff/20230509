# Comparing `tmp/torchpippy-0.1.0-py3-none-any.whl.zip` & `tmp/torchpippy-0.1.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,126 +1,127 @@
-Zip file size: 378040 bytes, number of entries: 124
--rw-rw-r--  2.0 unx    45482 b- defN 23-Apr-18 23:53 pippy/IR.py
--rw-rw-r--  2.0 unx     6691 b- defN 23-Apr-18 23:53 pippy/LoadModule.py
--rw-rw-r--  2.0 unx     6977 b- defN 23-Apr-18 23:53 pippy/ModelSplit.py
--rw-rw-r--  2.0 unx    88535 b- defN 23-Apr-18 23:53 pippy/PipelineDriver.py
--rw-rw-r--  2.0 unx      829 b- defN 23-Apr-18 23:53 pippy/__init__.py
--rw-rw-r--  2.0 unx    10405 b- defN 23-Apr-18 23:53 pippy/auto_parallelization.py
--rw-rw-r--  2.0 unx     3803 b- defN 23-Apr-18 23:53 pippy/backward.py
--rw-rw-r--  2.0 unx     5944 b- defN 23-Apr-18 23:53 pippy/compile.py
--rw-rw-r--  2.0 unx     3230 b- defN 23-Apr-18 23:53 pippy/events.py
--rw-rw-r--  2.0 unx    12063 b- defN 23-Apr-18 23:53 pippy/microbatch.py
--rw-rw-r--  2.0 unx     9291 b- defN 23-Apr-18 23:53 pippy/utils.py
--rw-rw-r--  2.0 unx       79 b- defN 23-Apr-18 23:53 pippy/version.py
--rw-rw-r--  2.0 unx     5126 b- defN 23-Apr-18 23:53 pippy/visualizer.py
--rw-rw-r--  2.0 unx     3830 b- defN 23-Apr-18 23:53 pippy/fx/__init__.py
--rw-rw-r--  2.0 unx     1052 b- defN 23-Apr-18 23:53 pippy/fx/_compatibility.py
--rw-rw-r--  2.0 unx     1950 b- defN 23-Apr-18 23:53 pippy/fx/_pytree.py
--rw-rw-r--  2.0 unx    41325 b- defN 23-Apr-18 23:53 pippy/fx/_symbolic_trace.py
--rw-rw-r--  2.0 unx      981 b- defN 23-Apr-18 23:53 pippy/fx/annotate.py
--rw-rw-r--  2.0 unx    61274 b- defN 23-Apr-18 23:53 pippy/fx/graph.py
--rw-rw-r--  2.0 unx    31380 b- defN 23-Apr-18 23:53 pippy/fx/graph_module.py
--rw-rw-r--  2.0 unx     2253 b- defN 23-Apr-18 23:53 pippy/fx/immutable_collections.py
--rw-rw-r--  2.0 unx    20313 b- defN 23-Apr-18 23:53 pippy/fx/interpreter.py
--rw-rw-r--  2.0 unx    27424 b- defN 23-Apr-18 23:53 pippy/fx/node.py
--rw-rw-r--  2.0 unx    18555 b- defN 23-Apr-18 23:53 pippy/fx/operator_schemas.py
--rw-rw-r--  2.0 unx    17005 b- defN 23-Apr-18 23:53 pippy/fx/proxy.py
--rw-rw-r--  2.0 unx    10333 b- defN 23-Apr-18 23:53 pippy/fx/subgraph_rewriter.py
--rw-rw-r--  2.0 unx     2988 b- defN 23-Apr-18 23:53 pippy/fx/tensor_type.py
--rw-rw-r--  2.0 unx     1490 b- defN 23-Apr-18 23:53 pippy/fx/traceback.py
--rw-rw-r--  2.0 unx       52 b- defN 23-Apr-18 23:53 pippy/fx/experimental/__init__.py
--rw-rw-r--  2.0 unx    48034 b- defN 23-Apr-18 23:53 pippy/fx/experimental/accelerator_partitioner.py
--rw-rw-r--  2.0 unx    12129 b- defN 23-Apr-18 23:53 pippy/fx/experimental/const_fold.py
--rw-rw-r--  2.0 unx      857 b- defN 23-Apr-18 23:53 pippy/fx/experimental/debug.py
--rw-rw-r--  2.0 unx    32747 b- defN 23-Apr-18 23:53 pippy/fx/experimental/graph_gradual_typechecker.py
--rw-rw-r--  2.0 unx     6043 b- defN 23-Apr-18 23:53 pippy/fx/experimental/merge_matmul.py
--rw-rw-r--  2.0 unx    10067 b- defN 23-Apr-18 23:53 pippy/fx/experimental/meta_tracer.py
--rw-rw-r--  2.0 unx     5530 b- defN 23-Apr-18 23:53 pippy/fx/experimental/normalize.py
--rw-rw-r--  2.0 unx    16596 b- defN 23-Apr-18 23:53 pippy/fx/experimental/optimization.py
--rw-rw-r--  2.0 unx    12596 b- defN 23-Apr-18 23:53 pippy/fx/experimental/partitioner_utils.py
--rw-rw-r--  2.0 unx    26734 b- defN 23-Apr-18 23:53 pippy/fx/experimental/proxy_tensor.py
--rw-rw-r--  2.0 unx      457 b- defN 23-Apr-18 23:53 pippy/fx/experimental/refinement_types.py
--rw-rw-r--  2.0 unx     4995 b- defN 23-Apr-18 23:53 pippy/fx/experimental/rewriter.py
--rw-rw-r--  2.0 unx     5052 b- defN 23-Apr-18 23:53 pippy/fx/experimental/schema_type_annotation.py
--rw-rw-r--  2.0 unx    17092 b- defN 23-Apr-18 23:53 pippy/fx/experimental/symbolic_shapes.py
--rw-rw-r--  2.0 unx     3173 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unify_refinements.py
--rw-rw-r--  2.0 unx       52 b- defN 23-Apr-18 23:53 pippy/fx/experimental/migrate_gradual_types/__init__.py
--rw-rw-r--  2.0 unx    16573 b- defN 23-Apr-18 23:53 pippy/fx/experimental/migrate_gradual_types/constraint.py
--rw-rw-r--  2.0 unx    48039 b- defN 23-Apr-18 23:53 pippy/fx/experimental/migrate_gradual_types/constraint_generator.py
--rw-rw-r--  2.0 unx    39454 b- defN 23-Apr-18 23:53 pippy/fx/experimental/migrate_gradual_types/constraint_transformation.py
--rw-rw-r--  2.0 unx      282 b- defN 23-Apr-18 23:53 pippy/fx/experimental/migrate_gradual_types/operation.py
--rw-rw-r--  2.0 unx    14754 b- defN 23-Apr-18 23:53 pippy/fx/experimental/migrate_gradual_types/transform_to_z3.py
--rw-rw-r--  2.0 unx     1373 b- defN 23-Apr-18 23:53 pippy/fx/experimental/migrate_gradual_types/util.py
--rw-rw-r--  2.0 unx      858 b- defN 23-Apr-18 23:53 pippy/fx/experimental/migrate_gradual_types/z3_types.py
--rw-rw-r--  2.0 unx      237 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/__init__.py
--rw-rw-r--  2.0 unx     2768 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/core.py
--rw-rw-r--  2.0 unx      243 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/dispatch.py
--rw-rw-r--  2.0 unx     3445 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/match.py
--rw-rw-r--  2.0 unx     2968 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/more.py
--rw-rw-r--  2.0 unx    10629 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/unification_tools.py
--rw-rw-r--  2.0 unx     2963 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/utils.py
--rw-rw-r--  2.0 unx     2014 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/variable.py
--rw-rw-r--  2.0 unx      197 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/multipledispatch/__init__.py
--rw-rw-r--  2.0 unx     4127 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/multipledispatch/conflict.py
--rw-rw-r--  2.0 unx     2536 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/multipledispatch/core.py
--rw-rw-r--  2.0 unx    13925 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/multipledispatch/dispatcher.py
--rw-rw-r--  2.0 unx     3830 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/multipledispatch/utils.py
--rw-rw-r--  2.0 unx     2992 b- defN 23-Apr-18 23:53 pippy/fx/experimental/unification/multipledispatch/variadic.py
--rw-rw-r--  2.0 unx      353 b- defN 23-Apr-18 23:53 pippy/fx/passes/__init__.py
--rw-rw-r--  2.0 unx     1064 b- defN 23-Apr-18 23:53 pippy/fx/passes/fake_tensor_prop.py
--rw-rw-r--  2.0 unx    12884 b- defN 23-Apr-18 23:53 pippy/fx/passes/graph_drawer.py
--rw-rw-r--  2.0 unx     4033 b- defN 23-Apr-18 23:53 pippy/fx/passes/graph_manipulation.py
--rw-rw-r--  2.0 unx    21774 b- defN 23-Apr-18 23:53 pippy/fx/passes/net_min_base.py
--rw-rw-r--  2.0 unx     7327 b- defN 23-Apr-18 23:53 pippy/fx/passes/operator_support.py
--rw-rw-r--  2.0 unx     3579 b- defN 23-Apr-18 23:53 pippy/fx/passes/param_fetch.py
--rw-rw-r--  2.0 unx     6792 b- defN 23-Apr-18 23:53 pippy/fx/passes/pass_manager.py
--rw-rw-r--  2.0 unx    32899 b- defN 23-Apr-18 23:53 pippy/fx/passes/reinplace.py
--rw-rw-r--  2.0 unx     5228 b- defN 23-Apr-18 23:53 pippy/fx/passes/shape_prop.py
--rw-rw-r--  2.0 unx    13863 b- defN 23-Apr-18 23:53 pippy/fx/passes/split_module.py
--rw-rw-r--  2.0 unx    10330 b- defN 23-Apr-18 23:53 pippy/fx/passes/split_utils.py
--rw-rw-r--  2.0 unx    31775 b- defN 23-Apr-18 23:53 pippy/fx/passes/splitter_base.py
--rw-rw-r--  2.0 unx     9579 b- defN 23-Apr-18 23:53 pippy/fx/passes/tools_common.py
--rw-rw-r--  2.0 unx       52 b- defN 23-Apr-18 23:53 pippy/fx/passes/backends/__init__.py
--rw-rw-r--  2.0 unx     2023 b- defN 23-Apr-18 23:53 pippy/fx/passes/backends/cudagraphs.py
--rw-rw-r--  2.0 unx    13451 b- defN 23-Apr-18 23:53 pippy/fx/passes/backends/nvfuser.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Apr-18 23:53 pippy/fx/passes/dialect/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Apr-18 23:53 pippy/fx/passes/dialect/common/__init__.py
--rw-rw-r--  2.0 unx     4979 b- defN 23-Apr-18 23:53 pippy/fx/passes/dialect/common/cse_pass.py
--rw-rw-r--  2.0 unx       79 b- defN 23-Apr-18 23:53 pippy/fx/passes/infra/__init__.py
--rw-rw-r--  2.0 unx    10270 b- defN 23-Apr-18 23:53 pippy/fx/passes/infra/partitioner.py
--rw-rw-r--  2.0 unx     2560 b- defN 23-Apr-18 23:53 pippy/fx/passes/infra/pass_base.py
--rw-rw-r--  2.0 unx    10120 b- defN 23-Apr-18 23:53 pippy/fx/passes/infra/pass_manager.py
--rw-rw-r--  2.0 unx       52 b- defN 23-Apr-18 23:53 pippy/fx/passes/tests/__init__.py
--rw-rw-r--  2.0 unx     1158 b- defN 23-Apr-18 23:53 pippy/fx/passes/tests/test_pass_manager.py
--rw-rw-r--  2.0 unx      110 b- defN 23-Apr-18 23:53 pippy/fx/passes/utils/__init__.py
--rw-rw-r--  2.0 unx     2869 b- defN 23-Apr-18 23:53 pippy/fx/passes/utils/common.py
--rw-rw-r--  2.0 unx     7567 b- defN 23-Apr-18 23:53 pippy/fx/passes/utils/fuser_utils.py
--rw-rw-r--  2.0 unx    13419 b- defN 23-Apr-18 23:53 pippy/fx/passes/utils/matcher_utils.py
--rw-rw-r--  2.0 unx      418 b- defN 23-Apr-18 23:53 pippy/hf/__init__.py
--rw-rw-r--  2.0 unx     1587 b- defN 23-Apr-18 23:53 pippy/hf/bart.py
--rw-rw-r--  2.0 unx      948 b- defN 23-Apr-18 23:53 pippy/hf/bert.py
--rw-rw-r--  2.0 unx      920 b- defN 23-Apr-18 23:53 pippy/hf/gpt2.py
--rw-rw-r--  2.0 unx      943 b- defN 23-Apr-18 23:53 pippy/hf/roberta.py
--rw-rw-r--  2.0 unx     1536 b- defN 23-Apr-18 23:53 pippy/hf/t5.py
--rw-rw-r--  2.0 unx    10958 b- defN 23-Apr-18 23:53 pippy/hf/utils.py
--rw-rw-r--  2.0 unx       52 b- defN 23-Apr-18 23:53 test/__init__.py
--rw-rw-r--  2.0 unx    39680 b- defN 23-Apr-18 23:53 test/hf_test.py
--rw-rw-r--  2.0 unx     6273 b- defN 23-Apr-18 23:53 test/local_test_autosplit.py
--rw-rw-r--  2.0 unx     2900 b- defN 23-Apr-18 23:53 test/local_test_compile.py
--rw-rw-r--  2.0 unx     7499 b- defN 23-Apr-18 23:53 test/local_test_ddp.py
--rw-rw-r--  2.0 unx     4826 b- defN 23-Apr-18 23:53 test/local_test_forward.py
--rw-rw-r--  2.0 unx     5052 b- defN 23-Apr-18 23:53 test/local_test_forward_auto_parallel.py
--rw-rw-r--  2.0 unx    11061 b- defN 23-Apr-18 23:53 test/local_test_forward_backward.py
--rw-rw-r--  2.0 unx     4465 b- defN 23-Apr-18 23:53 test/local_test_forward_hf_bert.py
--rw-rw-r--  2.0 unx     4445 b- defN 23-Apr-18 23:53 test/local_test_forward_hf_gpt2.py
--rw-rw-r--  2.0 unx     3271 b- defN 23-Apr-18 23:53 test/local_test_null_coalesce_accumulate.py
--rw-rw-r--  2.0 unx    12726 b- defN 23-Apr-18 23:53 test/local_test_visualizer.py
--rw-rw-r--  2.0 unx     5143 b- defN 23-Apr-18 23:53 test/min_gpt_tracing.py
--rw-rw-r--  2.0 unx   153949 b- defN 23-Apr-18 23:53 test/test_fx.py
--rw-rw-r--  2.0 unx    60953 b- defN 23-Apr-18 23:53 test/test_fx_experimental.py
--rw-rw-r--  2.0 unx    27837 b- defN 23-Apr-18 23:53 test/test_ir.py
--rw-rw-r--  2.0 unx     1525 b- defN 23-Apr-18 23:53 torchpippy-0.1.0.dist-info/LICENSE
--rw-rw-r--  2.0 unx      588 b- defN 23-Apr-18 23:53 torchpippy-0.1.0.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-18 23:53 torchpippy-0.1.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx       11 b- defN 23-Apr-18 23:53 torchpippy-0.1.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    11154 b- defN 23-Apr-18 23:53 torchpippy-0.1.0.dist-info/RECORD
-124 files, 1428022 bytes uncompressed, 360322 bytes compressed:  74.8%
+Zip file size: 381792 bytes, number of entries: 125
+-rw-rw-r--  2.0 unx    46865 b- defN 23-May-09 21:15 pippy/IR.py
+-rw-rw-r--  2.0 unx     7759 b- defN 23-May-09 21:15 pippy/LoadModule.py
+-rw-rw-r--  2.0 unx     6977 b- defN 23-May-09 21:15 pippy/ModelSplit.py
+-rw-rw-r--  2.0 unx    89001 b- defN 23-May-09 21:15 pippy/PipelineDriver.py
+-rw-rw-r--  2.0 unx      829 b- defN 23-May-09 21:15 pippy/__init__.py
+-rw-rw-r--  2.0 unx    10405 b- defN 23-May-09 21:15 pippy/auto_parallelization.py
+-rw-rw-r--  2.0 unx     3803 b- defN 23-May-09 21:15 pippy/backward.py
+-rw-rw-r--  2.0 unx     6010 b- defN 23-May-09 21:15 pippy/compile.py
+-rw-rw-r--  2.0 unx     3230 b- defN 23-May-09 21:15 pippy/events.py
+-rw-rw-r--  2.0 unx    14010 b- defN 23-May-09 21:15 pippy/microbatch.py
+-rw-rw-r--  2.0 unx     9291 b- defN 23-May-09 21:15 pippy/utils.py
+-rw-rw-r--  2.0 unx       79 b- defN 23-May-09 21:17 pippy/version.py
+-rw-rw-r--  2.0 unx     5126 b- defN 23-May-09 21:15 pippy/visualizer.py
+-rw-rw-r--  2.0 unx     3830 b- defN 23-May-09 21:15 pippy/fx/__init__.py
+-rw-rw-r--  2.0 unx     1052 b- defN 23-May-09 21:15 pippy/fx/_compatibility.py
+-rw-rw-r--  2.0 unx     1950 b- defN 23-May-09 21:15 pippy/fx/_pytree.py
+-rw-rw-r--  2.0 unx    41325 b- defN 23-May-09 21:15 pippy/fx/_symbolic_trace.py
+-rw-rw-r--  2.0 unx      981 b- defN 23-May-09 21:15 pippy/fx/annotate.py
+-rw-rw-r--  2.0 unx    61274 b- defN 23-May-09 21:15 pippy/fx/graph.py
+-rw-rw-r--  2.0 unx    31380 b- defN 23-May-09 21:15 pippy/fx/graph_module.py
+-rw-rw-r--  2.0 unx     2253 b- defN 23-May-09 21:15 pippy/fx/immutable_collections.py
+-rw-rw-r--  2.0 unx    20313 b- defN 23-May-09 21:15 pippy/fx/interpreter.py
+-rw-rw-r--  2.0 unx    27424 b- defN 23-May-09 21:15 pippy/fx/node.py
+-rw-rw-r--  2.0 unx    18555 b- defN 23-May-09 21:15 pippy/fx/operator_schemas.py
+-rw-rw-r--  2.0 unx    17005 b- defN 23-May-09 21:15 pippy/fx/proxy.py
+-rw-rw-r--  2.0 unx    10333 b- defN 23-May-09 21:15 pippy/fx/subgraph_rewriter.py
+-rw-rw-r--  2.0 unx     2988 b- defN 23-May-09 21:15 pippy/fx/tensor_type.py
+-rw-rw-r--  2.0 unx     1490 b- defN 23-May-09 21:15 pippy/fx/traceback.py
+-rw-rw-r--  2.0 unx       52 b- defN 23-May-09 21:15 pippy/fx/experimental/__init__.py
+-rw-rw-r--  2.0 unx    48034 b- defN 23-May-09 21:15 pippy/fx/experimental/accelerator_partitioner.py
+-rw-rw-r--  2.0 unx    12129 b- defN 23-May-09 21:15 pippy/fx/experimental/const_fold.py
+-rw-rw-r--  2.0 unx      857 b- defN 23-May-09 21:15 pippy/fx/experimental/debug.py
+-rw-rw-r--  2.0 unx    32747 b- defN 23-May-09 21:15 pippy/fx/experimental/graph_gradual_typechecker.py
+-rw-rw-r--  2.0 unx     6043 b- defN 23-May-09 21:15 pippy/fx/experimental/merge_matmul.py
+-rw-rw-r--  2.0 unx    10067 b- defN 23-May-09 21:15 pippy/fx/experimental/meta_tracer.py
+-rw-rw-r--  2.0 unx     5530 b- defN 23-May-09 21:15 pippy/fx/experimental/normalize.py
+-rw-rw-r--  2.0 unx    16596 b- defN 23-May-09 21:15 pippy/fx/experimental/optimization.py
+-rw-rw-r--  2.0 unx    12596 b- defN 23-May-09 21:15 pippy/fx/experimental/partitioner_utils.py
+-rw-rw-r--  2.0 unx    26734 b- defN 23-May-09 21:15 pippy/fx/experimental/proxy_tensor.py
+-rw-rw-r--  2.0 unx      457 b- defN 23-May-09 21:15 pippy/fx/experimental/refinement_types.py
+-rw-rw-r--  2.0 unx     4995 b- defN 23-May-09 21:15 pippy/fx/experimental/rewriter.py
+-rw-rw-r--  2.0 unx     5052 b- defN 23-May-09 21:15 pippy/fx/experimental/schema_type_annotation.py
+-rw-rw-r--  2.0 unx    17092 b- defN 23-May-09 21:15 pippy/fx/experimental/symbolic_shapes.py
+-rw-rw-r--  2.0 unx     3173 b- defN 23-May-09 21:15 pippy/fx/experimental/unify_refinements.py
+-rw-rw-r--  2.0 unx       52 b- defN 23-May-09 21:15 pippy/fx/experimental/migrate_gradual_types/__init__.py
+-rw-rw-r--  2.0 unx    16573 b- defN 23-May-09 21:15 pippy/fx/experimental/migrate_gradual_types/constraint.py
+-rw-rw-r--  2.0 unx    48039 b- defN 23-May-09 21:15 pippy/fx/experimental/migrate_gradual_types/constraint_generator.py
+-rw-rw-r--  2.0 unx    39454 b- defN 23-May-09 21:15 pippy/fx/experimental/migrate_gradual_types/constraint_transformation.py
+-rw-rw-r--  2.0 unx      282 b- defN 23-May-09 21:15 pippy/fx/experimental/migrate_gradual_types/operation.py
+-rw-rw-r--  2.0 unx    14754 b- defN 23-May-09 21:15 pippy/fx/experimental/migrate_gradual_types/transform_to_z3.py
+-rw-rw-r--  2.0 unx     1373 b- defN 23-May-09 21:15 pippy/fx/experimental/migrate_gradual_types/util.py
+-rw-rw-r--  2.0 unx      858 b- defN 23-May-09 21:15 pippy/fx/experimental/migrate_gradual_types/z3_types.py
+-rw-rw-r--  2.0 unx      237 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/__init__.py
+-rw-rw-r--  2.0 unx     2768 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/core.py
+-rw-rw-r--  2.0 unx      243 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/dispatch.py
+-rw-rw-r--  2.0 unx     3445 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/match.py
+-rw-rw-r--  2.0 unx     2968 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/more.py
+-rw-rw-r--  2.0 unx    10629 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/unification_tools.py
+-rw-rw-r--  2.0 unx     2963 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/utils.py
+-rw-rw-r--  2.0 unx     2014 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/variable.py
+-rw-rw-r--  2.0 unx      197 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/multipledispatch/__init__.py
+-rw-rw-r--  2.0 unx     4127 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/multipledispatch/conflict.py
+-rw-rw-r--  2.0 unx     2536 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/multipledispatch/core.py
+-rw-rw-r--  2.0 unx    13925 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/multipledispatch/dispatcher.py
+-rw-rw-r--  2.0 unx     3830 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/multipledispatch/utils.py
+-rw-rw-r--  2.0 unx     2992 b- defN 23-May-09 21:15 pippy/fx/experimental/unification/multipledispatch/variadic.py
+-rw-rw-r--  2.0 unx      353 b- defN 23-May-09 21:15 pippy/fx/passes/__init__.py
+-rw-rw-r--  2.0 unx     1064 b- defN 23-May-09 21:15 pippy/fx/passes/fake_tensor_prop.py
+-rw-rw-r--  2.0 unx    12884 b- defN 23-May-09 21:15 pippy/fx/passes/graph_drawer.py
+-rw-rw-r--  2.0 unx     4033 b- defN 23-May-09 21:15 pippy/fx/passes/graph_manipulation.py
+-rw-rw-r--  2.0 unx    21774 b- defN 23-May-09 21:15 pippy/fx/passes/net_min_base.py
+-rw-rw-r--  2.0 unx     7327 b- defN 23-May-09 21:15 pippy/fx/passes/operator_support.py
+-rw-rw-r--  2.0 unx     3579 b- defN 23-May-09 21:15 pippy/fx/passes/param_fetch.py
+-rw-rw-r--  2.0 unx     6792 b- defN 23-May-09 21:15 pippy/fx/passes/pass_manager.py
+-rw-rw-r--  2.0 unx    32899 b- defN 23-May-09 21:15 pippy/fx/passes/reinplace.py
+-rw-rw-r--  2.0 unx     5228 b- defN 23-May-09 21:15 pippy/fx/passes/shape_prop.py
+-rw-rw-r--  2.0 unx    13863 b- defN 23-May-09 21:15 pippy/fx/passes/split_module.py
+-rw-rw-r--  2.0 unx    10330 b- defN 23-May-09 21:15 pippy/fx/passes/split_utils.py
+-rw-rw-r--  2.0 unx    31775 b- defN 23-May-09 21:15 pippy/fx/passes/splitter_base.py
+-rw-rw-r--  2.0 unx     9579 b- defN 23-May-09 21:15 pippy/fx/passes/tools_common.py
+-rw-rw-r--  2.0 unx       52 b- defN 23-May-09 21:15 pippy/fx/passes/backends/__init__.py
+-rw-rw-r--  2.0 unx     2023 b- defN 23-May-09 21:15 pippy/fx/passes/backends/cudagraphs.py
+-rw-rw-r--  2.0 unx    13451 b- defN 23-May-09 21:15 pippy/fx/passes/backends/nvfuser.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-09 21:15 pippy/fx/passes/dialect/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-09 21:15 pippy/fx/passes/dialect/common/__init__.py
+-rw-rw-r--  2.0 unx     4979 b- defN 23-May-09 21:15 pippy/fx/passes/dialect/common/cse_pass.py
+-rw-rw-r--  2.0 unx       79 b- defN 23-May-09 21:15 pippy/fx/passes/infra/__init__.py
+-rw-rw-r--  2.0 unx    10270 b- defN 23-May-09 21:15 pippy/fx/passes/infra/partitioner.py
+-rw-rw-r--  2.0 unx     2560 b- defN 23-May-09 21:15 pippy/fx/passes/infra/pass_base.py
+-rw-rw-r--  2.0 unx    10120 b- defN 23-May-09 21:15 pippy/fx/passes/infra/pass_manager.py
+-rw-rw-r--  2.0 unx       52 b- defN 23-May-09 21:15 pippy/fx/passes/tests/__init__.py
+-rw-rw-r--  2.0 unx     1158 b- defN 23-May-09 21:15 pippy/fx/passes/tests/test_pass_manager.py
+-rw-rw-r--  2.0 unx      110 b- defN 23-May-09 21:15 pippy/fx/passes/utils/__init__.py
+-rw-rw-r--  2.0 unx     2869 b- defN 23-May-09 21:15 pippy/fx/passes/utils/common.py
+-rw-rw-r--  2.0 unx     7567 b- defN 23-May-09 21:15 pippy/fx/passes/utils/fuser_utils.py
+-rw-rw-r--  2.0 unx    13419 b- defN 23-May-09 21:15 pippy/fx/passes/utils/matcher_utils.py
+-rw-rw-r--  2.0 unx      418 b- defN 23-May-09 21:15 pippy/hf/__init__.py
+-rw-rw-r--  2.0 unx     1587 b- defN 23-May-09 21:15 pippy/hf/bart.py
+-rw-rw-r--  2.0 unx      948 b- defN 23-May-09 21:15 pippy/hf/bert.py
+-rw-rw-r--  2.0 unx      920 b- defN 23-May-09 21:15 pippy/hf/gpt2.py
+-rw-rw-r--  2.0 unx      943 b- defN 23-May-09 21:15 pippy/hf/roberta.py
+-rw-rw-r--  2.0 unx     1536 b- defN 23-May-09 21:15 pippy/hf/t5.py
+-rw-rw-r--  2.0 unx    10958 b- defN 23-May-09 21:15 pippy/hf/utils.py
+-rw-rw-r--  2.0 unx       52 b- defN 23-May-09 21:15 test/__init__.py
+-rw-rw-r--  2.0 unx    39680 b- defN 23-May-09 21:15 test/hf_test.py
+-rw-rw-r--  2.0 unx     6273 b- defN 23-May-09 21:15 test/local_test_autosplit.py
+-rw-rw-r--  2.0 unx     4404 b- defN 23-May-09 21:15 test/local_test_c10d.py
+-rw-rw-r--  2.0 unx     2900 b- defN 23-May-09 21:15 test/local_test_compile.py
+-rw-rw-r--  2.0 unx     7499 b- defN 23-May-09 21:15 test/local_test_ddp.py
+-rw-rw-r--  2.0 unx     4826 b- defN 23-May-09 21:15 test/local_test_forward.py
+-rw-rw-r--  2.0 unx     5052 b- defN 23-May-09 21:15 test/local_test_forward_auto_parallel.py
+-rw-rw-r--  2.0 unx    11061 b- defN 23-May-09 21:15 test/local_test_forward_backward.py
+-rw-rw-r--  2.0 unx     4465 b- defN 23-May-09 21:15 test/local_test_forward_hf_bert.py
+-rw-rw-r--  2.0 unx     4445 b- defN 23-May-09 21:15 test/local_test_forward_hf_gpt2.py
+-rw-rw-r--  2.0 unx     3271 b- defN 23-May-09 21:15 test/local_test_null_coalesce_accumulate.py
+-rw-rw-r--  2.0 unx    12726 b- defN 23-May-09 21:15 test/local_test_visualizer.py
+-rw-rw-r--  2.0 unx     5143 b- defN 23-May-09 21:15 test/min_gpt_tracing.py
+-rw-rw-r--  2.0 unx   153949 b- defN 23-May-09 21:15 test/test_fx.py
+-rw-rw-r--  2.0 unx    60953 b- defN 23-May-09 21:15 test/test_fx_experimental.py
+-rw-rw-r--  2.0 unx    30084 b- defN 23-May-09 21:15 test/test_ir.py
+-rw-rw-r--  2.0 unx     1525 b- defN 23-May-09 21:17 torchpippy-0.1.1.dist-info/LICENSE
+-rw-rw-r--  2.0 unx      691 b- defN 23-May-09 21:17 torchpippy-0.1.1.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-09 21:17 torchpippy-0.1.1.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       11 b- defN 23-May-09 21:17 torchpippy-0.1.1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    11234 b- defN 23-May-09 21:17 torchpippy-0.1.1.dist-info/RECORD
+125 files, 1439786 bytes uncompressed, 363952 bytes compressed:  74.7%
```

## zipnote {}

```diff
@@ -312,14 +312,17 @@
 
 Filename: test/hf_test.py
 Comment: 
 
 Filename: test/local_test_autosplit.py
 Comment: 
 
+Filename: test/local_test_c10d.py
+Comment: 
+
 Filename: test/local_test_compile.py
 Comment: 
 
 Filename: test/local_test_ddp.py
 Comment: 
 
 Filename: test/local_test_forward.py
@@ -351,23 +354,23 @@
 
 Filename: test/test_fx_experimental.py
 Comment: 
 
 Filename: test/test_ir.py
 Comment: 
 
-Filename: torchpippy-0.1.0.dist-info/LICENSE
+Filename: torchpippy-0.1.1.dist-info/LICENSE
 Comment: 
 
-Filename: torchpippy-0.1.0.dist-info/METADATA
+Filename: torchpippy-0.1.1.dist-info/METADATA
 Comment: 
 
-Filename: torchpippy-0.1.0.dist-info/WHEEL
+Filename: torchpippy-0.1.1.dist-info/WHEEL
 Comment: 
 
-Filename: torchpippy-0.1.0.dist-info/top_level.txt
+Filename: torchpippy-0.1.1.dist-info/top_level.txt
 Comment: 
 
-Filename: torchpippy-0.1.0.dist-info/RECORD
+Filename: torchpippy-0.1.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pippy/IR.py

```diff
@@ -1,12 +1,13 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates
 import copy
 import logging
 import operator
 from enum import Enum
+import os
 import threading
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 
 import torch
 import torch.fx as torch_fx
 import pippy.fx
 from packaging import version
@@ -487,24 +488,63 @@
     serialization_dict.pop("_graph")
     return (
         _direct_serialization_deserialize,
         (serialization_dict, _LinearNodeList(self.graph.nodes)),
     )
 
 
-class Pipe(torch.nn.Module):
+class QualnameMapMixin:
+    """
+    A mixin class to provide qualname remap functionality for both Pipe object
+    and submodules
+    """
+
+    def __init__(
+        self,
+        qualname_mapping: Dict[str, str] = None,
+    ):
+        self.new_to_old_qualname_mapping: Dict[str, str] = (
+            qualname_mapping or {}
+        )
+
+    def remap_qualname(self, qualname: str):
+        # TODO: annoying
+        if qualname.startswith("split_gm."):
+            qualname = qualname[len("split_gm.") :]
+
+        # The qualname map does not store recursive items, thus,
+        # when passed a qualname with leaves, we need to perform longest prefix match
+        if qualname not in self.new_to_old_qualname_mapping:
+            # Split from the right, one each time
+            split_names = qualname.rsplit(".", 1)
+            leaf = split_names[-1]
+            while len(split_names) > 1:
+                prefix = split_names[0]
+                if prefix in self.new_to_old_qualname_mapping:
+                    old_prefix = self.new_to_old_qualname_mapping[prefix]
+                    return ".".join([old_prefix, leaf])
+                split_names = prefix.rsplit(".", 1)
+                leaf = ".".join([split_names[-1], leaf])
+
+        # Either full name match, or key not found
+        return self.new_to_old_qualname_mapping[qualname]
+
+
+class Pipe(QualnameMapMixin, torch.nn.Module):
     def __init__(
         self,
         split_gm: pippy.fx.GraphModule,
         qualname_mapping: Dict[str, str],
         num_stages: int,
         has_loss_and_backward: bool,
         loss_spec,
     ):
-        super().__init__()
+        # TODO: is there a way not to hard wire init?
+        QualnameMapMixin.__init__(self, qualname_mapping)
+        torch.nn.Module.__init__(self)
         self.split_gm: pippy.fx.GraphModule = split_gm
         self.executor: DetachExecutor = DetachExecutor(self.split_gm)
         self.num_stages: int = num_stages
         self.has_loss_and_backwards = has_loss_and_backward
         self.loss_spec = loss_spec
         self.last_grads = None
 
@@ -552,15 +592,29 @@
                 atoms = param_qualname.split(".")
                 for atom in atoms[:-1]:
                     submod = getattr(submod, atom)
                 setattr(
                     submod, atoms[-1], copy.deepcopy(getattr(submod, atoms[-1]))
                 )
 
-        self.new_to_old_qualname_mapping = qualname_mapping
+        # Create qualname mapping for each submodule
+        for m_qualname, mod in self.split_gm.named_children():
+            # "submod_x." prefix
+            mod_prefix = m_qualname + "."
+            mod_qualname_mapping: Dict[str, str] = {}
+            for k, v in self.new_to_old_qualname_mapping.items():
+                if k.startswith(mod_prefix):
+                    # Remove prefix
+                    new_key = k[len(mod_prefix) :]
+                    mod_qualname_mapping.setdefault(new_key, v)
+            # Add a remap mixin to submodule instance
+            mod.__class__ = type(
+                "PipeStageModule", (QualnameMapMixin, mod.__class__), {}
+            )
+            setattr(mod, "new_to_old_qualname_mapping", mod_qualname_mapping)
 
         def throw(self, *args, **kwargs):
             raise RuntimeError(
                 "To run pipeline locally, invoke the Pipe object directly, not `split_gm`"
             )
 
         self.split_gm.forward = throw  # type: ignore
@@ -610,36 +664,14 @@
         res = self.executor.run(*executor_args)
 
         if self.has_loss_and_backwards:
             res, self.last_grads = res
 
         return res
 
-    def remap_qualname(self, qualname):
-        # TODO: annoying
-        if qualname.startswith("split_gm."):
-            qualname = qualname[len("split_gm.") :]
-
-        # The qualname map does not store recursive items, thus,
-        # when passed a qualname with leaves, we need to perform longest prefix match
-        if qualname not in self.new_to_old_qualname_mapping:
-            # Split from the right, one each time
-            split_names = qualname.rsplit(".", 1)
-            leaf = split_names[-1]
-            while len(split_names) > 1:
-                prefix = split_names[0]
-                if prefix in self.new_to_old_qualname_mapping:
-                    old_prefix = self.new_to_old_qualname_mapping[prefix]
-                    return ".".join([old_prefix, leaf])
-                split_names = prefix.rsplit(".", 1)
-                leaf = ".".join([split_names[-1], leaf])
-
-        # Either full name match, or key not found
-        return self.new_to_old_qualname_mapping[qualname]
-
     @staticmethod
     def _number_and_count_forward_stages(gm: pippy.fx.GraphModule):
         num_stages = 0
         found_idxs: Dict[int, None] = {}
         for node in gm.graph.nodes:
             if node.op == "call_module" and node.target.startswith("submod_"):
                 node.meta["stage_idx"] = int(node.target[len("submod_") :])
@@ -1047,24 +1079,31 @@
         return self.split_gm.__repr__()
 
     # Conditoinal variable to ensure `defer_stage_init` is called before other callers call `materialize_stage`
     # TODO: cleaner approach
     _stage_init_lock = threading.Lock()
     stage_init_cv = threading.Condition(_stage_init_lock)
 
-    def defer_stage_init(self, device, index_filename=None, dtype=None):
+    def defer_stage_init(
+        self,
+        device: torch.device,
+        index_filename: Union[str, os.PathLike] = None,
+        dtype: torch.dtype = None,
+        checkpoint_prefix: str = None,
+    ):
         def materialize_stage(target: str) -> torch.nn.Module:
             logging.info(f"Materializing {target} on {device}")
             submodule = self.split_gm.get_submodule(target)
             if index_filename is not None:
                 submodule = load_checkpoint(
                     model=submodule,
                     index_filename=index_filename,
                     device=device,
                     dtype=dtype,
+                    checkpoint_prefix=checkpoint_prefix,
                 )
             try:
                 submodule.to(device)
             except Exception:
                 # Usually `to(device)` fails because there is still some meta
                 # tensor in submodule, potentially because the checkpoint load
                 # did not cover that parameter. And the reason is often that
```

## pippy/LoadModule.py

```diff
@@ -1,172 +1,233 @@
 import os
 import json
 import gc
-from typing import Optional, Union
+from typing import Dict, List, Optional, Tuple, Union
+import logging
 
 import torch
 from torch import nn
 
 
+TYPICAL_PREFIXES = [
+    "model",  # facebook/opt-6.7b
+    "transformer",  # bigscience/bloom-7b1
+]
+
+
 def load_checkpoint(
     model: nn.Module,
     index_filename: Union[str, os.PathLike],
     device: torch.device = None,
     dtype: torch.dtype = None,
+    checkpoint_prefix: str = None,
 ):
     checkpoint_folder = os.path.split(index_filename)[0]
     with open(index_filename, "r") as f:
         index = json.loads(f.read())
     if "weight_map" in index:
         index = index["weight_map"]
-    checkpoint_files = sorted(list(set(index.values())))
-    checkpoint_files = [
-        os.path.join(checkpoint_folder, f) for f in checkpoint_files
-    ]
-    for checkpoint_file in checkpoint_files:
-        checkpoint = torch.load(checkpoint_file)
-        for param_name, param in checkpoint.items():
-            # Some weights like word_embeddings.weight and shared.weight will be used in different layers, but these layers
-            # may not in the index file, so we can only clone the shared weight to their corresponding layers.
-            if param_name in [
-                "word_embeddings.weight",
-                "shared.weight",
-                "wte.weight",
-            ]:
-                if hasattr(model, "lm_head"):
-                    model.lm_head.weight = torch.nn.Parameter(  # type: ignore[union-attr]
-                        (param.clone()).to(device).to(dtype)
-                    )
-                if hasattr(model, "encoder_embed_tokens"):
-                    model.encoder_embed_tokens.weight = torch.nn.Parameter(  # type: ignore[union-attr]
-                        (param.clone()).to(device).to(dtype)
-                    )
-                if hasattr(model, "decoder_embed_tokens"):
-                    model.decoder_embed_tokens.weight = torch.nn.Parameter(  # type: ignore[union-attr]
-                        (param.clone()).to(device).to(dtype)
-                    )
-            elif param_name in [
-                "decoder.embed_tokens.weight",
-            ]:
-                # For OPT, the lm_head weight is automatically tied to the embed tokens weight
-                if hasattr(model, "lm_head"):
-                    model.lm_head.weight = torch.nn.Parameter(  # type: ignore[union-attr]
-                        (param.clone()).to(device).to(dtype)
-                    )
-            set_module_tensor_to_device(
-                model, param_name, device, value=param, dtype=dtype
-            )
+
+    prefix_to_test = (
+        [checkpoint_prefix] if checkpoint_prefix else TYPICAL_PREFIXES
+    )
+
+    file_to_weights = _get_file_to_weight_map(model, index, prefix_to_test)
+
+    used_files = file_to_weights.keys()
+    import time
+
+    logging.info(
+        f"Timestamp {time.time():.2f} " f"Opening checkpoint: {used_files}"
+    )
+
+    for file in used_files:
+        file_path = os.path.join(checkpoint_folder, file)
+        checkpoint = torch.load(file_path)
+
+        if file in file_to_weights:
+            weights = file_to_weights[file]
+            for new_name, old_name, clone in weights:
+                assert (
+                    old_name in checkpoint.keys()
+                ), f"{old_name} not in {file}"
+                loaded_weight = checkpoint[old_name]
+                _set_module_tensor_to_device(
+                    model,
+                    new_name,
+                    device,
+                    value=loaded_weight,
+                    dtype=dtype,
+                    clone=clone,
+                )
+
         del checkpoint
         gc.collect()
 
     return model
 
 
-def set_module_tensor_to_device(
+def _get_file_to_weight_map(
+    model: nn.Module,
+    index,
+    prefix_to_test: List[str],
+) -> Dict[str, List[Tuple]]:
+    file_to_weights: Dict[str, List[Tuple]] = {}
+
+    for iterator in [
+        model.named_parameters(),
+        model.named_buffers(),
+    ]:
+        for new_name, _ in iterator:
+            old_name = model.remap_qualname(new_name)  # type: ignore[operator]
+            cp_weight_name, clone_needed = _match_checkpoint_name(
+                old_name, index, prefix_to_test
+            )
+            if cp_weight_name is None:
+                raise RuntimeError(
+                    f"Weight {new_name} maps to {old_name}, "
+                    f"but {old_name} is not found in checkpoint index"
+                )
+            file = index[cp_weight_name]
+            weights = file_to_weights.setdefault(file, [])
+            weights.append((new_name, cp_weight_name, clone_needed))
+
+    return file_to_weights
+
+
+# Some weights like word_embeddings.weight and shared.weight will be used in
+# different layers, but these layers may not in the index file, so we can only
+# clone the shared weight to their corresponding layers.
+
+TYPICAL_SHARER_WEIGHTS = [
+    "lm_head.weight",  # facebook/opt-6.7b
+    "encoder_embed_tokens.weight",
+]
+
+TYPICAL_SHAREE_WEIGHTS = [
+    "decoder.embed_tokens.weight",
+    "word_embeddings.weight",
+    "shared.weight",
+    "wte.weight",
+]
+
+
+def _match_checkpoint_name(
+    old_name: str,
+    index,
+    prefix_to_test: List[str],
+) -> Tuple[Optional[str], bool]:
+    """
+    A helper function to match weight name against those in checkpoint index.
+    Args:
+        old_name (`str`):
+            weight name in original model (retrieved via `remap_qualname()`)
+        index (`Dict`?):
+            checkpoint index
+        prefix_to_test (`List[str]`):
+            prefix to try if direct match is not found
+    Return:
+        weight name in the checkpoint index and whether clone is needed
+    Search rule:
+        - Exact match, no need to clone
+        - Match after prefix, no need to clone
+        - Match via shared weight table, clone needed
+    """
+    if old_name in index.keys():
+        return old_name, False
+
+    for prefix in prefix_to_test:
+        if (
+            old_name.startswith(prefix)
+            and old_name[len(prefix) + 1 :] in index.keys()
+        ):
+            return old_name[len(prefix) + 1 :], False
+
+    if old_name in TYPICAL_SHARER_WEIGHTS:
+        for sharee in TYPICAL_SHAREE_WEIGHTS:
+            if sharee in index.keys():
+                return sharee, True
+
+    return None, False
+
+
+def _set_module_tensor_to_device(
     module: nn.Module,
-    param_name: str,
+    qualname: str,
     device: Optional[torch.device] = None,
     value: Optional[torch.Tensor] = None,
-    dtype: Optional[Union[str, torch.dtype]] = None,
+    dtype: Optional[torch.dtype] = None,
+    clone: bool = False,
 ):
     """
     A helper function to set a given tensor (parameter of buffer) of a module on a specific device (note that doing
     `param.to(device)` creates a new tensor not linked to the parameter, which is why we need this function).
     Args:
         module (`torch.nn.Module`):
             The module in which the tensor we want to move lives.
-        param_name (`str`):
+        qualname (`str`):
             The full name of the parameter/buffer.
         device (`torch.device`):
             The device on which to set the tensor.
         value (`torch.Tensor`, *optional*):
             The value of the tensor (useful when going from the meta device to any other device).
         dtype (`torch.dtype`, *optional*):
             If passed along the value of the parameter will be cast to this `dtype`. Otherwise, `value` will be cast to
             the dtype of the existing parameter in the model.
+        clone (`bool`, default is False):
+            whether to copy the input value.
     """
     # Recurse if needed
     if value is None:
         return
-    # Parameters are renamed by tracing
-    model_pipe_module_name = "_".join(param_name.split(".")[:-1])
-    model_pipe_tensor_name = param_name.split(".")[-1]
-    # Parameters in module
-    normal_params = [
-        model_pipe_module_name,
-        "model_" + model_pipe_module_name,
-        "transformer_" + model_pipe_module_name,
-    ]
-    # Parameters in module._parameters
-    moved_params = [
-        "moved_" + model_pipe_module_name,
-        "moved_model_" + model_pipe_module_name,
-        "moved_transformer_" + model_pipe_module_name,
-    ]
-    tensor_name = None
-    for param in normal_params:
-        if hasattr(module, param):
-            module = getattr(module, param)
-            tensor_name = model_pipe_tensor_name
-            break
-    if tensor_name is None:
-        for param in moved_params:
-            param = param + "_" + model_pipe_tensor_name
-            if param in module._parameters.keys():
-                tensor_name = param
-                break
-    if tensor_name is None:
-        return
 
-    if (
-        tensor_name not in module._parameters
-        and tensor_name not in module._buffers
-    ):
-        raise ValueError(
-            f"{module} does not have a parameter or a buffer named {tensor_name}."
-        )
-    is_buffer = tensor_name in module._buffers
-    old_value = getattr(module, tensor_name)
+    submod_and_weight = qualname.rsplit(".", 1)
+    if len(submod_and_weight) > 1:
+        submod = getattr(module, submod_and_weight[0])
+        weight = submod_and_weight[1]
+    else:
+        submod = module
+        weight = submod_and_weight[0]
 
-    if (
-        old_value.device == torch.device("meta")
-        and device not in [None, torch.device("meta")]
-        and value is None
-    ):
+    if weight not in submod._parameters and weight not in submod._buffers:
         raise ValueError(
-            f"{tensor_name} is on the meta device, we need a `value` to put in on {device}."
+            f"{submod._get_name()} does not have a parameter or a buffer named {weight}. "
+            f"Has instead: {submod._parameters.keys()} and {submod._buffers.keys()}"
         )
 
-    if value is not None:
-        if dtype is None:
-            # For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model
-            value = value.to(old_value.dtype)
-        elif not str(value.dtype).startswith(
-            ("torch.uint", "torch.int", "torch.bool")
-        ):
-            value = value.to(dtype)
+    is_buffer = weight in submod._buffers
+    old_value = getattr(submod, weight)
+
+    if dtype is None:
+        # For compatibility with PyTorch load_state_dict which converts state
+        # dict dtype to existing dtype in model
+        dtype = old_value.dtype
+    elif str(value.dtype).startswith(("torch.uint", "torch.int", "torch.bool")):
+        # Avoid casting these data types
+        dtype = value.dtype
 
     with torch.no_grad():
-        if value is None:
-            new_value = old_value.to(device)
-        elif isinstance(value, torch.Tensor):
-            new_value = value.to(device)
+        if isinstance(value, torch.Tensor):
+            new_value = value.to(device=device, dtype=dtype)
+            # In case device and dtype do not change, `new_value` would point to
+            # the same storage as `value`, and we would need to explicitly clone
+            if clone and new_value.data_ptr() == value.data_ptr():
+                new_value = value.clone()
         else:
-            new_value = torch.tensor(value, device=device)
+            # Note: `torch.tensor()` allocates new memory to copy the data of
+            # tensor, so clone is taken care of
+            new_value = torch.tensor(value, device=device, dtype=dtype)
 
         if is_buffer:
-            module._buffers[tensor_name] = new_value
-        elif value is not None or device not in [
-            None,
-            module._parameters[tensor_name].device,
-        ]:
-            param_cls = type(module._parameters[tensor_name])
-            kwargs = module._parameters[tensor_name].__dict__
+            submod._buffers[weight] = new_value
+        else:
+            param_cls = type(submod._parameters[weight])
+            kwargs = submod._parameters[weight].__dict__
             if param_cls.__name__ == "Int8Params":
-                new_value = param_cls(  # type: ignore[misc]
+                new_param = param_cls(  # type: ignore[misc]
                     new_value, requires_grad=old_value.requires_grad, **kwargs
-                ).to(device)
+                )
             else:
-                new_value = param_cls(  # type: ignore[misc]
+                new_param = param_cls(  # type: ignore[misc]
                     new_value, requires_grad=old_value.requires_grad
-                ).to(device)
-            module._parameters[tensor_name] = new_value.to(dtype)  # type: ignore[assignment]
+                )
+            submod._parameters[weight] = new_param
```

## pippy/PipelineDriver.py

```diff
@@ -1474,36 +1474,41 @@
             self.rank_worker_rrefs[rank] = rpc.remote(
                 rank, RankWorker, args=(), kwargs=kwargs
             )
             pp_rank += 1
 
         self.stage_to_executor: Dict = {}
 
+        # Ask each RankWorker to create stage thereon
+        # This can involve checkpoint loading in deferred init case
         for stage_id, descr in enumerate(executor_descriptors):
             # Assign stages to rank workers in a round-robin fashion
             rank = self.all_ranks[stage_id % self.world_size]
             logging.debug(f"[root] Sending stage_id = {stage_id} mod to worker")
             self.remote_stage_executor_rrefs[descr.name] = (
                 stage_id,
                 self.rank_worker_rrefs[rank]
                 .remote()
                 .create_stage_executor(
                     stage_id=stage_id,
                     mod=descr.mod,
                     mod_name=descr.name,
                 ),
             )
-            if Pipe.is_stage_init_deferred():
-                logging.debug(
-                    f"[root] Waiting stage_id = {stage_id} mod to be confirmed by worker"
-                )
-                while not self.remote_stage_executor_rrefs[descr.name][
-                    1
-                ].confirmed_by_owner():
-                    pass
+
+        # Check that each RankWorker has completed stage init
+        for stage_id, descr in enumerate(executor_descriptors):
+            logging.debug(
+                f"[root] Waiting stage_id = {stage_id} mod to be confirmed by worker"
+            )
+            while not self.remote_stage_executor_rrefs[descr.name][
+                1
+            ].confirmed_by_owner():
+                pass
+
             self.stage_to_executor[stage_id] = self.remote_stage_executor_rrefs[
                 descr.name
             ][1]
 
         # Inform executors of their peers
         for stage_id, executor in self.stage_to_executor.items():
             executor.rpc_sync().install_peer_executors(self.stage_to_executor)
@@ -2081,17 +2086,14 @@
 
         self._init_remote_executors()
 
     def forward(self, *args, **kwargs):
         if self.single_loss:
             raise NotImplementedError("Single minibatch loss not implemented")
 
-        logging.info(
-            f"[root] Running pipeline with {self.chunks} micro-batches"
-        )
         # Roadmap:
         # 1) Micro-batch splitting - divide input arguments out into concrete chunk values
         # 2) Interpreter tiling - one interpreter per micro-batch
         # 3) Scheduling - Use control logic to advance interpreters to issue round-robin
         #       forward work items, then round-robin losses, then round-robin backwards
 
         args_split, kwargs_split = split_args_kwargs_into_chunks(
@@ -2099,32 +2101,44 @@
             kwargs,
             self.chunks,
             self.args_chunk_spec,
             self.kwargs_chunk_spec,
             self._debug_mask_minibatches,
         )
 
+        real_num_chunks = self.chunks
+        if len(args_split) < self.chunks:
+            real_num_chunks = len(args_split)
+            warnings.warn(
+                f"Reducing micro-batch numbers from {self.chunks} to "
+                f"{real_num_chunks}."
+            )
+
+        logging.info(
+            f"[root] Running pipeline with {real_num_chunks} micro-batches"
+        )
+
         self.microbatch_interpreters = []
 
         batch_id = self.batch_id
         self.batch_id += 1
 
-        for chunk in range(self.chunks):
+        for chunk in range(real_num_chunks):
             logging.debug(
                 f"[root] Instantiating microbatch interpreter for chunk {chunk}"
             )
             interp = RemoteInterpreter(
                 remote_stage_executor_rrefs=self.remote_stage_executor_rrefs,
                 stage_to_executor=self.stage_to_executor,
                 module=self.pipe.split_gm,
                 cur_microbatch=chunk,
                 args=args_split[chunk],
                 kwargs=kwargs_split[chunk],
                 batch_id=batch_id,
-                num_microbatches=self.chunks,
+                num_microbatches=real_num_chunks,
             )
             # If user wants to use c10d for P2P, we would perform the shape propagation here. The shape prop is
             # performed per batch, thus supporting dynamic shape in batch dimension. Dynamic shape in microbatch
             # dimension is not yet supported, because all RemoteInterpreters share the same shape info (since they share
             # the same split_gm)
             if self.use_c10d and chunk == 0:
                 interp.propagate_shape(args_split[chunk], kwargs_split[chunk])
```

## pippy/compile.py

```diff
@@ -49,14 +49,15 @@
     loss_reducer: LossReducer = sum_reducer,
     args_chunk_spec=None,
     kwargs_chunk_spec=None,
     output_chunk_spec=None,
     checkpoint=False,
     _debug_mask_minibatches: bool = False,
     index_filename=None,
+    checkpoint_prefix: str = None,
     **kwargs,
 ):
     if ranks is None:
         ranks = list(range(num_ranks))
 
     if all_compile:
         rank = get_rank()
@@ -96,14 +97,15 @@
         if hasattr(mod, "config") and hasattr(mod.config, "torch_dtype"):
             dtype = mod.config.torch_dtype  # type: ignore[union-attr]
 
         pipe_model.defer_stage_init(
             device,
             index_filename,
             dtype,
+            checkpoint_prefix,
         )
         stage_mod = pipe_model.export(pp_rank)
 
     if pp_rank == 0:
         logging.info(pipe_model.split_gm)
 
         logging.info("[PiPPy] Creating pipeline driver ...")
```

## pippy/microbatch.py

```diff
@@ -1,10 +1,11 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates
 import logging
 from typing import Any
+import warnings
 from pippy.IR import TrivialLossWrapper
 import torch
 
 from torch.utils._pytree import tree_flatten, tree_unflatten
 
 
 class CustomReducer:
@@ -46,17 +47,19 @@
     num_chunks,
     _debug_mask_minibatches: bool = False,
 ):
     # Stage 1+2: flatten and shard/replicate
 
     # args_sharded_replicated : [num args, num flat values, num chunks]
     args_sharded_replicated = {}
-
     arg_specs = []
 
+    real_num_chunks = num_chunks
+    first_tensor = True
+
     assert len(args_dict) == len(
         args_chunk_spec
     ), f"args_dict.keys() = {list(args_dict.keys())} args_chunk_spec.keys() = {list(args_chunk_spec.keys())}"
 
     for arg_key, arg in args_dict.items():
         flat, spec = tree_flatten(arg)
         arg_specs.append(spec)
@@ -73,23 +76,42 @@
             # If user did not provide an args_chunk_spec, we would use a default spec which chunks along dim 0
             chunk_spec_flat = [TensorChunkSpec(DEFAULT_CHUNK_DIM)] * len(flat)
 
         sharded_arg_flat = []
 
         for v, chunk_v in zip(flat, chunk_spec_flat):
             if chunk_v is Replicate or not isinstance(v, torch.Tensor):
-                sharded_arg_flat.append([v] * num_chunks)
+                sharded_arg_flat.append([v] * real_num_chunks)
             elif isinstance(chunk_v, TensorChunkSpec):
                 # TODO: check type of v. If it's a tensor, use chunk (or debug mask).
                 # If it's a collection type, split it as you would expect. Otherwise,
                 # Throw an error
                 assert isinstance(v, torch.Tensor), f"{v} is not a tensor"
 
+                v_split_dim_size = v.size(chunk_v.split_dim)
+                if v_split_dim_size < real_num_chunks:
+                    if first_tensor:
+                        # We can only adjust number of chunks when we hit this
+                        # issue at the first tensor encountered
+                        warnings.warn(
+                            f"Tensor size on chunking dimension is {v_split_dim_size}, "
+                            f"downsizing the number of chunks from {num_chunks} to {v_split_dim_size}."
+                        )
+                        real_num_chunks = v_split_dim_size
+                    else:
+                        raise RuntimeError(
+                            f"Arg {arg_key} on chunking dimension has a size of {v_split_dim_size}, "
+                            f"smaller than the number of chunks {num_chunks}. "
+                            "PiPPy cannot reduce the number of chunks because "
+                            "other arguments have bigger chunk-dimension sizes. "
+                            "Please adjust your num_chunks setting."
+                        )
+
                 chunk_tensors = torch.tensor_split(
-                    v, num_chunks, chunk_v.split_dim
+                    v, real_num_chunks, chunk_v.split_dim
                 )
 
                 if _debug_mask_minibatches:
                     expanded_chunks = []
 
                     split_dim_idx = 0
                     for chunk_tensor in chunk_tensors:
@@ -107,22 +129,24 @@
                         expanded_chunks.append(new_val)
 
                         split_dim_idx += chunk_tensor.size(chunk_v.split_dim)
 
                     sharded_arg_flat.append(expanded_chunks)
                 else:
                     sharded_arg_flat.append(chunk_tensors)
+
+                first_tensor = False
             else:
                 raise TypeError(f"Unrecognized chunk spec: {chunk_v}")
 
         args_sharded_replicated[arg_key] = sharded_arg_flat
 
     # chunks_flat : [num chunks, num args, num flat values]
     chunks_flat = []
-    for chunk_idx in range(num_chunks):
+    for chunk_idx in range(real_num_chunks):
         chunk_args = {}
         for key, arg in args_sharded_replicated.items():
             arg_single_chunk = []
             for v_flat in arg:
                 arg_single_chunk.append(v_flat[chunk_idx])
             chunk_args[key] = arg_single_chunk
         chunks_flat.append(chunk_args)
@@ -192,19 +216,38 @@
 
     args_split_dict = shard_dict_of_args(
         dict(enumerate(args)),
         dict(enumerate(args_chunk_spec)),
         chunks,
         _debug_mask_minibatches,
     )
+    real_num_chunks = len(args_split_dict)
 
     kwargs_split = shard_dict_of_args(
-        kwargs, kwargs_chunk_spec, chunks, _debug_mask_minibatches
+        kwargs, kwargs_chunk_spec, real_num_chunks, _debug_mask_minibatches
     )
 
+    if len(kwargs_split) < real_num_chunks:
+        # In case kwargs are sharded into less chunks
+        # e.g. when `args` has no tensor, just values
+        real_num_chunks = len(kwargs_split)
+        # Re-shard args
+        args_split_dict = shard_dict_of_args(
+            dict(enumerate(args)),
+            dict(enumerate(args_chunk_spec)),
+            real_num_chunks,
+            _debug_mask_minibatches,
+        )
+
+    if len(args_split_dict) != len(kwargs_split):
+        raise RuntimeError(
+            "args and kwargs are split into different number of chunks: "
+            f"{len(args_split_dict)}, {len(kwargs_split)}"
+        )
+
     args_split = []
     for chunk_args in args_split_dict:
         args_split.append(tuple(chunk_args[i] for i in range(len(chunk_args))))
 
     return args_split, kwargs_split
```

## pippy/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '0.1.0'
-git_version = 'afcad6659a370cea61c50f97e5fb4b3256d1a99e'
+__version__ = '0.1.1'
+git_version = 'c0f29edeff6e8b15ac5b0783a4d389aa38ce4e29'
```

## test/test_ir.py

```diff
@@ -51,24 +51,25 @@
         x = x + skip_connection
         x = torch.mm(x, self.mm_param2)
         x = self.lin(x)
         return x
 
 
 def check_qualname_mapping(old, new):
-    seen_old_qns = {}
+    old_names = {}
+    # setdefault can de-duplicate the old names in the mapping
     for _, old_qn in new.new_to_old_qualname_mapping.items():
-        seen_old_qns.setdefault(old_qn)
+        old_names.setdefault(old_qn)
 
     # Do not check recursive parameter names as they don't exist in the mapping
     # The resursive names will be checked by tests with the remap_qualname call
     for param_name, _ in old.named_parameters(recurse=False):
         assert (
-            param_name in seen_old_qns
-        ), f"Expected parameter {param_name} in {seen_old_qns}"
+            param_name in old_names
+        ), f"Expected parameter {param_name} in {old_names}"
 
 
 class TestIR(unittest.TestCase):
     def setUp(self):
         mods = [torch.nn.Linear(512, 512) for _ in range(5)]
         mods += [mods[0]]
         self.seq = torch.nn.Sequential(*mods)
@@ -111,14 +112,19 @@
             "submod_2.lin": "lin",
             "submod_1.moved_buffer": "buffer",
             "submod_2.moved_mm_param2": "mm_param2",
             "submod_0.moved_mm_param": "mm_param",
         }
         self.assertDictEqual(expected_map, ec_pipe.new_to_old_qualname_mapping)
 
+        # Check remap_qualname method
+        for k in expected_map.keys():
+            old_name = ec_pipe.remap_qualname(k)
+            self.assertEqual(expected_map[k], old_name)
+
     def test_tracing_replicate(self):
         ec_pipe_replicated = Pipe.from_tracing(
             self.ec, MultiUseParameterConfig.REPLICATE
         )
         x = torch.randn(5, 512)
         torch.testing.assert_close(self.ec(x), ec_pipe_replicated(x))
         assert ec_pipe_replicated.replicated_params == [
@@ -804,10 +810,56 @@
         torch.testing.assert_close(
             chunks_merged_masked["added"], ref_out["added"]
         )
         torch.testing.assert_close(
             chunks_merged_masked["multiplied"], ref_out["multiplied"]
         )
 
+    def test_remap_qualname_transmit(self):
+        ec_pipe = Pipe.from_tracing(self.ec, MultiUseParameterConfig.TRANSMIT)
+
+        # Get the first field of all tuples, i.e. names
+        old_named_params = zip(*list(self.ec.named_parameters()))
+        old_names = list(old_named_params)[0]
+
+        # Check qualname mapping for pipe
+        for new_name, _ in ec_pipe.named_parameters():
+            old_name = ec_pipe.remap_qualname(new_name)
+            # print(f"{new_name} -> {old_name}")
+            assert (
+                old_name in old_names
+            ), f"Remapped parameter {old_name} not found in {old_names}"
+
+        # Check qualname mapping for submodule
+        for _, stage_mod in ec_pipe.split_gm.named_children():
+            for new_name, _ in stage_mod.named_parameters():
+                old_name = stage_mod.remap_qualname(new_name)
+                assert (
+                    old_name in old_names
+                ), f"Remapped parameter {old_name} not found in {old_names}"
+
+    def test_remap_qualname_replicate(self):
+        ec_pipe = Pipe.from_tracing(self.ec, MultiUseParameterConfig.REPLICATE)
+
+        # Get the first field of all tuples, i.e. names
+        old_named_params = zip(*list(self.ec.named_parameters()))
+        old_names = list(old_named_params)[0]
+
+        # Check qualname mapping for pipe
+        for new_name, _ in ec_pipe.named_parameters():
+            old_name = ec_pipe.remap_qualname(new_name)
+            # print(f"{new_name} -> {old_name}")
+            assert (
+                old_name in old_names
+            ), f"Remapped parameter {old_name} not found in {old_names}"
+
+        # Check qualname mapping for submodule
+        for _, stage_mod in ec_pipe.split_gm.named_children():
+            for new_name, _ in stage_mod.named_parameters():
+                old_name = stage_mod.remap_qualname(new_name)
+                assert (
+                    old_name in old_names
+                ), f"Remapped parameter {old_name} not found in {old_names}"
+
 
 if __name__ == "__main__":
     unittest.main()
```

## Comparing `torchpippy-0.1.0.dist-info/LICENSE` & `torchpippy-0.1.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `torchpippy-0.1.0.dist-info/METADATA` & `torchpippy-0.1.1.dist-info/METADATA`

 * *Files 23% similar despite different names*

```diff
@@ -1,17 +1,21 @@
 Metadata-Version: 2.1
 Name: torchpippy
-Version: 0.1.0
+Version: 0.1.1
 Summary: Pipeline Parallelism for PyTorch
 Home-page: https://github.com/pytorch/PiPPy
 Author: PiPPy Team
 License: BSD
+Platform: UNKNOWN
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: torch (>=1.13.0)
 
 
 The PiPPy project stands for Pipeline Parallelism for PyTorch. It consists of a
 compiler and runtime stack for automated parallelism and scaling of PyTorch
 models. PiPPy partitions the code of the model in a pipelined fashion and
 enables multiple micro-batches to execute different parts of the model code
-concurrently.
+concurrently. For details, please visit PiPPy's [GitHub
+page](https://github.com/pytorch/PiPPy).
+
+
```

## Comparing `torchpippy-0.1.0.dist-info/RECORD` & `torchpippy-0.1.1.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-pippy/IR.py,sha256=IwmaiDLvhOC52WEJ1kVoJILOUQY7pGU-G2uU6I5mG3s,45482
-pippy/LoadModule.py,sha256=NHca98oHFV8AknL_kxHWiodTiEe1G4RHFNEkGYGgxIw,6691
+pippy/IR.py,sha256=0bEfzrUWk3onj8JKHeGfpcl0fWB-Jl7iYGGfXkW3-AY,46865
+pippy/LoadModule.py,sha256=7VrE1mEPtjMQEDqstzRkPYRmDXV-SVKCPmIoPKLPGkY,7759
 pippy/ModelSplit.py,sha256=NeDpyXniV9DerVybQwoCW6QmWriH2RLXXYkTDlsnljA,6977
-pippy/PipelineDriver.py,sha256=aaZYwlKEJ9mZhivtFqLUpI9sWZmFHjw2vtI6BESuMTU,88535
+pippy/PipelineDriver.py,sha256=1IgxkiMc0DJLWzoxOQNeUGCaBISZ5exlNvkue3nmKYk,89001
 pippy/__init__.py,sha256=SzWs8CSE2-ONn2TF07SQWee9awbyzNAbWr9kidzWLCo,829
 pippy/auto_parallelization.py,sha256=Y25qLV9Lx30NOGMdSTJGn_QLmzN8VfwjvAhj9fCB_ic,10405
 pippy/backward.py,sha256=nXvfKqAUIpaVcR4HhawgZdXWRSJaBib0M0csoKH_Am8,3803
-pippy/compile.py,sha256=IeNb94V5d6Hg1JOYw15Co4OXkUnuH1H5Hd6X1fClzYQ,5944
+pippy/compile.py,sha256=2Y9FKYMS0M6zGbS_pyVpoeNnACa0X2pMfu0NF9qiCkA,6010
 pippy/events.py,sha256=nxXO7U2pk2bH6Cb3y-oNLc9aTzX-b2XrnLINAHnURNw,3230
-pippy/microbatch.py,sha256=DlV-GP6koNXhU31wca0cKBJNrJu0eC2b-BQRNCaRpJo,12063
+pippy/microbatch.py,sha256=9LdJIBWEA3l1ssXtWtvXjAtn3lIp1Fr17H1vSEUm69E,14010
 pippy/utils.py,sha256=wmZmWz9VD_331rcnrWI3sZvYyJTHMcT4eK33Uv4EpNE,9291
-pippy/version.py,sha256=njYepCgItPh2ffT0dApTROXT0XY8srJly9v_iRvNJcU,79
+pippy/version.py,sha256=el60I3PpvjjveKBkE4Q1w4GYYOl12t41-_XtHu-Iyeo,79
 pippy/visualizer.py,sha256=SLJe9vfdwGXhIR_J906aiwNtZEaSgQ89nUjPqYgcbec,5126
 pippy/fx/__init__.py,sha256=BDYVelOx8xfqh8IhvNHKE25zhees9FJvy55-UQ41BC0,3830
 pippy/fx/_compatibility.py,sha256=msIZiCfVg4bve8c0JVfj_rCtbDSeT_hq_Ge382xNEaU,1052
 pippy/fx/_pytree.py,sha256=aF7ZoxF0ELOCJdsSqv3sXck7dKbWI6n0Oh2RcmN8xn4,1950
 pippy/fx/_symbolic_trace.py,sha256=wuYOPY5BC-gCDhOmAmC9UpAFh7wlYYY24PdLjSBK0v0,41325
 pippy/fx/annotate.py,sha256=VMLplAjMi8aaoA6O_CTeXq8CjO-yjaTTR0c15Ktba-4,981
 pippy/fx/graph.py,sha256=XOwLlcCsTsvxhXB7Wz6zG2LDjqDKkk4sV1KwsBzUJEU,61274
@@ -100,25 +100,26 @@
 pippy/hf/gpt2.py,sha256=o9nMXoqcZ5NTDtpUsAxCw1acedE_kOJYnDODXYnvKIE,920
 pippy/hf/roberta.py,sha256=Bqy-SNbuwVGLoWa2aoWT0qELxDB1siVKRQ1kYOTJQ5c,943
 pippy/hf/t5.py,sha256=7BMfHnWtyoN9Hym__OLRgTtjSLafbomCYF16ej3OLr8,1536
 pippy/hf/utils.py,sha256=-Q4Dg0CH17L1MDZyNrQvSg9OJ6bYqhGk4f328pIggCg,10958
 test/__init__.py,sha256=jLQo34kgGOSuumAbVsa8rQRJtt8LZKnXvqpTvpsNSjo,52
 test/hf_test.py,sha256=hu0wYv_9KDACMUiwYqsJN-LJIDqnZKHxwOKUgK_gyy0,39680
 test/local_test_autosplit.py,sha256=N3Z9jmBYqWn_2BVBwvF7ERjRWe9M__HHMKNQ2jlCsRQ,6273
+test/local_test_c10d.py,sha256=oWq0tA0kpQ_ZZ6aTOt4OVKUr5SJFPZP-WzkveQSTr9c,4404
 test/local_test_compile.py,sha256=Md0tKzAYnmEl3PsLwK_mPPho9HfVz-YzNGS52dGUzn8,2900
 test/local_test_ddp.py,sha256=zACjwJTkYuQvR4pmxX7yJxrzvau3LSWYhfVK-56jdZw,7499
 test/local_test_forward.py,sha256=lk35iP8r6AWidXknR7YGUJtuZ2rXD7KeT7HGHGySuWA,4826
 test/local_test_forward_auto_parallel.py,sha256=3ZFj6ulARpMpP3Nr5zRVAWXLntN4sZGXqysp3apg66I,5052
 test/local_test_forward_backward.py,sha256=Wif0P28NrvEE-jjGP45sLYanklbNLMFp835WIRILN2U,11061
 test/local_test_forward_hf_bert.py,sha256=6OQnNzwA1ZJF1heORJzGfe2P1Mpoxr3UjJxk4dSWjAI,4465
 test/local_test_forward_hf_gpt2.py,sha256=1HaH7yyI7YQahsjRqWeuxgpBynq_8BwiiqtQDOjfza0,4445
 test/local_test_null_coalesce_accumulate.py,sha256=1idnuztHsvns7jRxBVJUTuOmpDxLqoBWbxT5xrC4AQk,3271
 test/local_test_visualizer.py,sha256=-lTzpx6Xdvod91zdqs_EzQnLV7X4y3dFhTpjOJOHDv8,12726
 test/min_gpt_tracing.py,sha256=AKL78jeT4P5A1xlcpLOR6GEPOO7WZjf_FCuTmgMEIKI,5143
 test/test_fx.py,sha256=HpSdaAvo253gPLXwqB7N3iVNfHTt3WNebESAx8h8DUU,153949
 test/test_fx_experimental.py,sha256=cROnpQPFqlY67d76WT51cS-0jHL9dewgjeINIYLUXTo,60953
-test/test_ir.py,sha256=BSe6Gz6wdwtw-RA16_aoeAhHgpVlyEjYsWfYzhUDNSo,27837
-torchpippy-0.1.0.dist-info/LICENSE,sha256=He6zNCXpMWSxIKL1begpOA9KewGPTTc7eOYftg_r2uo,1525
-torchpippy-0.1.0.dist-info/METADATA,sha256=T0hY9RF33FwtmT0OS9L-DZglXtJbQ-1Vgu3os55V_5E,588
-torchpippy-0.1.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-torchpippy-0.1.0.dist-info/top_level.txt,sha256=V5DWsxF_jZ9T-ZmKd3HDwKHdx6r42AeL1K77EZDDnBY,11
-torchpippy-0.1.0.dist-info/RECORD,,
+test/test_ir.py,sha256=q7NU6VBK9LcavV4L_UUsF_ZplPTAe7gIT4RtKKhzChQ,30084
+torchpippy-0.1.1.dist-info/LICENSE,sha256=He6zNCXpMWSxIKL1begpOA9KewGPTTc7eOYftg_r2uo,1525
+torchpippy-0.1.1.dist-info/METADATA,sha256=i5kmryONkv_igtu-VzFrnXw9hTrlgzH6u7Q6sVTEbYo,691
+torchpippy-0.1.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+torchpippy-0.1.1.dist-info/top_level.txt,sha256=V5DWsxF_jZ9T-ZmKd3HDwKHdx6r42AeL1K77EZDDnBY,11
+torchpippy-0.1.1.dist-info/RECORD,,
```

