# Comparing `tmp/google_generativeai-0.1.0rc1-py3-none-any.whl.zip` & `tmp/google_generativeai-0.1.0rc2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,75 +1,78 @@
-Zip file size: 117316 bytes, number of entries: 73
--rw-r--r--  2.0 unx      539 b- defN 23-May-03 23:00 google_generativeai-0.1.0rc1-py3.10-nspkg.pth
--rw-r--r--  2.0 unx     1835 b- defN 23-May-03 21:33 google/generativeai/__init__.py
--rw-r--r--  2.0 unx     5241 b- defN 23-May-03 21:33 google/generativeai/client.py
--rw-r--r--  2.0 unx    16965 b- defN 23-May-03 21:33 google/generativeai/discuss.py
--rw-r--r--  2.0 unx     2946 b- defN 23-May-03 21:33 google/generativeai/models.py
--rw-r--r--  2.0 unx     6932 b- defN 23-May-03 21:33 google/generativeai/text.py
--rw-r--r--  2.0 unx      624 b- defN 23-May-03 22:59 google/generativeai/version.py
--rw-r--r--  2.0 unx     1123 b- defN 23-May-03 22:14 google/generativeai/notebook/__init__.py
--rw-r--r--  2.0 unx     3791 b- defN 23-May-03 22:12 google/generativeai/notebook/argument_parser.py
--rw-r--r--  2.0 unx     1910 b- defN 23-May-03 22:12 google/generativeai/notebook/argument_parser_test.py
--rw-r--r--  2.0 unx    19612 b- defN 23-May-03 22:12 google/generativeai/notebook/cmd_line_parser.py
--rw-r--r--  2.0 unx    15213 b- defN 23-May-03 22:12 google/generativeai/notebook/cmd_line_parser_test.py
--rw-r--r--  2.0 unx     1503 b- defN 23-May-03 22:12 google/generativeai/notebook/command.py
--rw-r--r--  2.0 unx     6049 b- defN 23-May-03 22:12 google/generativeai/notebook/command_utils.py
--rw-r--r--  2.0 unx     2466 b- defN 23-May-03 22:12 google/generativeai/notebook/compare_cmd.py
--rw-r--r--  2.0 unx     2277 b- defN 23-May-03 22:12 google/generativeai/notebook/compile_cmd.py
--rw-r--r--  2.0 unx     2648 b- defN 23-May-03 22:12 google/generativeai/notebook/eval_cmd.py
--rw-r--r--  2.0 unx    16231 b- defN 23-May-03 22:12 google/generativeai/notebook/flag_def.py
--rw-r--r--  2.0 unx    14382 b- defN 23-May-03 22:12 google/generativeai/notebook/flag_def_test.py
--rw-r--r--  2.0 unx     7450 b- defN 23-May-03 22:12 google/generativeai/notebook/gspread_client.py
--rw-r--r--  2.0 unx     1513 b- defN 23-May-03 22:12 google/generativeai/notebook/html_utils.py
--rw-r--r--  2.0 unx     1802 b- defN 23-May-03 22:12 google/generativeai/notebook/html_utils_test.py
--rw-r--r--  2.0 unx     2743 b- defN 23-May-03 22:12 google/generativeai/notebook/input_utils.py
--rw-r--r--  2.0 unx     2396 b- defN 23-May-03 22:12 google/generativeai/notebook/input_utils_test.py
--rw-r--r--  2.0 unx     1636 b- defN 23-May-03 22:12 google/generativeai/notebook/ipython_env.py
--rw-r--r--  2.0 unx     1044 b- defN 23-May-03 22:12 google/generativeai/notebook/ipython_env_impl.py
--rw-r--r--  2.0 unx     3895 b- defN 23-May-03 22:12 google/generativeai/notebook/magics.py
--rw-r--r--  2.0 unx     5294 b- defN 23-May-03 22:12 google/generativeai/notebook/magics_engine.py
--rw-r--r--  2.0 unx    24832 b- defN 23-May-03 22:12 google/generativeai/notebook/magics_engine_test.py
--rw-r--r--  2.0 unx     1859 b- defN 23-May-03 22:12 google/generativeai/notebook/model_registry.py
--rw-r--r--  2.0 unx     1249 b- defN 23-May-03 22:12 google/generativeai/notebook/model_registry_test.py
--rw-r--r--  2.0 unx     2023 b- defN 23-May-03 22:12 google/generativeai/notebook/output_utils.py
--rw-r--r--  2.0 unx     3048 b- defN 23-May-03 22:12 google/generativeai/notebook/parsed_args_lib.py
--rw-r--r--  2.0 unx     5613 b- defN 23-May-03 22:12 google/generativeai/notebook/post_process_utils.py
--rw-r--r--  2.0 unx     8621 b- defN 23-May-03 22:12 google/generativeai/notebook/post_process_utils_test.py
--rw-r--r--  2.0 unx      985 b- defN 23-May-03 22:12 google/generativeai/notebook/post_process_utils_test_helper.py
--rw-r--r--  2.0 unx     2003 b- defN 23-May-03 22:12 google/generativeai/notebook/py_utils.py
--rw-r--r--  2.0 unx     1717 b- defN 23-May-03 22:12 google/generativeai/notebook/py_utils_test.py
--rw-r--r--  2.0 unx     2636 b- defN 23-May-03 22:12 google/generativeai/notebook/run_cmd.py
--rw-r--r--  2.0 unx     2901 b- defN 23-May-03 22:12 google/generativeai/notebook/sheets_id.py
--rw-r--r--  2.0 unx     2091 b- defN 23-May-03 22:12 google/generativeai/notebook/sheets_id_test.py
--rw-r--r--  2.0 unx     2933 b- defN 23-May-03 22:12 google/generativeai/notebook/sheets_sanitize_url.py
--rw-r--r--  2.0 unx     3771 b- defN 23-May-03 22:12 google/generativeai/notebook/sheets_sanitize_url_test.py
--rw-r--r--  2.0 unx     3808 b- defN 23-May-03 22:12 google/generativeai/notebook/sheets_utils.py
--rw-r--r--  2.0 unx     2114 b- defN 23-May-03 22:12 google/generativeai/notebook/text_model.py
--rw-r--r--  2.0 unx     3043 b- defN 23-May-03 22:12 google/generativeai/notebook/text_model_test.py
--rw-r--r--  2.0 unx      599 b- defN 23-May-03 22:14 google/generativeai/notebook/lib/__init__.py
--rw-r--r--  2.0 unx    16998 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/llm_function.py
--rw-r--r--  2.0 unx    14958 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/llm_function_test.py
--rw-r--r--  2.0 unx     2854 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/llmfn_input_utils.py
--rw-r--r--  2.0 unx     2312 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/llmfn_inputs_source.py
--rw-r--r--  2.0 unx     5543 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/llmfn_output_row.py
--rw-r--r--  2.0 unx     4963 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/llmfn_output_row_test.py
--rw-r--r--  2.0 unx     7888 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/llmfn_outputs.py
--rw-r--r--  2.0 unx     5763 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/llmfn_outputs_test.py
--rw-r--r--  2.0 unx     2468 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/llmfn_post_process.py
--rw-r--r--  2.0 unx     8058 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/llmfn_post_process_cmds.py
--rw-r--r--  2.0 unx     6720 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/llmfn_post_process_cmds_test.py
--rw-r--r--  2.0 unx     1969 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/model.py
--rw-r--r--  2.0 unx     1232 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/prompt_utils.py
--rw-r--r--  2.0 unx     1506 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/prompt_utils_test.py
--rw-r--r--  2.0 unx     1439 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/unique_fn.py
--rw-r--r--  2.0 unx     1778 b- defN 23-May-03 22:12 google/generativeai/notebook/lib/unique_fn_test.py
--rw-r--r--  2.0 unx      808 b- defN 23-May-03 21:33 google/generativeai/types/__init__.py
--rw-r--r--  2.0 unx     5640 b- defN 23-May-03 21:33 google/generativeai/types/discuss_types.py
--rw-r--r--  2.0 unx     2894 b- defN 23-May-03 21:33 google/generativeai/types/model_types.py
--rw-r--r--  2.0 unx     1313 b- defN 23-May-03 21:33 google/generativeai/types/text_types.py
--rw-r--r--  2.0 unx    11358 b- defN 23-May-03 23:00 google_generativeai-0.1.0rc1.dist-info/LICENSE
--rw-r--r--  2.0 unx     1289 b- defN 23-May-03 23:00 google_generativeai-0.1.0rc1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-03 23:00 google_generativeai-0.1.0rc1.dist-info/WHEEL
--rw-r--r--  2.0 unx        7 b- defN 23-May-03 23:00 google_generativeai-0.1.0rc1.dist-info/namespace_packages.txt
--rw-r--r--  2.0 unx        7 b- defN 23-May-03 23:00 google_generativeai-0.1.0rc1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     7445 b- defN 23-May-03 23:00 google_generativeai-0.1.0rc1.dist-info/RECORD
-73 files, 343208 bytes uncompressed, 105062 bytes compressed:  69.4%
+Zip file size: 121459 bytes, number of entries: 76
+-rw-r--r--  2.0 unx      539 b- defN 23-May-08 23:42 google_generativeai-0.1.0rc2-py3.10-nspkg.pth
+-rw-r--r--  2.0 unx     1977 b- defN 23-May-08 23:42 google/generativeai/__init__.py
+-rw-r--r--  2.0 unx     5677 b- defN 23-May-08 23:42 google/generativeai/client.py
+-rw-r--r--  2.0 unx    17625 b- defN 23-May-08 23:42 google/generativeai/discuss.py
+-rw-r--r--  2.0 unx      845 b- defN 23-May-08 23:42 google/generativeai/docstring_utils.py
+-rw-r--r--  2.0 unx     2946 b- defN 23-May-08 23:42 google/generativeai/models.py
+-rw-r--r--  2.0 unx     8209 b- defN 23-May-08 23:42 google/generativeai/text.py
+-rw-r--r--  2.0 unx      624 b- defN 23-May-08 23:42 google/generativeai/version.py
+-rw-r--r--  2.0 unx     1123 b- defN 23-May-08 23:42 google/generativeai/notebook/__init__.py
+-rw-r--r--  2.0 unx     3791 b- defN 23-May-08 23:42 google/generativeai/notebook/argument_parser.py
+-rw-r--r--  2.0 unx     1910 b- defN 23-May-08 23:42 google/generativeai/notebook/argument_parser_test.py
+-rw-r--r--  2.0 unx    19612 b- defN 23-May-08 23:42 google/generativeai/notebook/cmd_line_parser.py
+-rw-r--r--  2.0 unx    15213 b- defN 23-May-08 23:42 google/generativeai/notebook/cmd_line_parser_test.py
+-rw-r--r--  2.0 unx     1503 b- defN 23-May-08 23:42 google/generativeai/notebook/command.py
+-rw-r--r--  2.0 unx     6049 b- defN 23-May-08 23:42 google/generativeai/notebook/command_utils.py
+-rw-r--r--  2.0 unx     2466 b- defN 23-May-08 23:42 google/generativeai/notebook/compare_cmd.py
+-rw-r--r--  2.0 unx     2277 b- defN 23-May-08 23:42 google/generativeai/notebook/compile_cmd.py
+-rw-r--r--  2.0 unx     2648 b- defN 23-May-08 23:42 google/generativeai/notebook/eval_cmd.py
+-rw-r--r--  2.0 unx    16231 b- defN 23-May-08 23:42 google/generativeai/notebook/flag_def.py
+-rw-r--r--  2.0 unx    14382 b- defN 23-May-08 23:42 google/generativeai/notebook/flag_def_test.py
+-rw-r--r--  2.0 unx     7450 b- defN 23-May-08 23:42 google/generativeai/notebook/gspread_client.py
+-rw-r--r--  2.0 unx     1513 b- defN 23-May-08 23:42 google/generativeai/notebook/html_utils.py
+-rw-r--r--  2.0 unx     1802 b- defN 23-May-08 23:42 google/generativeai/notebook/html_utils_test.py
+-rw-r--r--  2.0 unx     2743 b- defN 23-May-08 23:42 google/generativeai/notebook/input_utils.py
+-rw-r--r--  2.0 unx     2396 b- defN 23-May-08 23:42 google/generativeai/notebook/input_utils_test.py
+-rw-r--r--  2.0 unx     1636 b- defN 23-May-08 23:42 google/generativeai/notebook/ipython_env.py
+-rw-r--r--  2.0 unx     1044 b- defN 23-May-08 23:42 google/generativeai/notebook/ipython_env_impl.py
+-rw-r--r--  2.0 unx     4156 b- defN 23-May-08 23:42 google/generativeai/notebook/magics.py
+-rw-r--r--  2.0 unx     5294 b- defN 23-May-08 23:42 google/generativeai/notebook/magics_engine.py
+-rw-r--r--  2.0 unx    24832 b- defN 23-May-08 23:42 google/generativeai/notebook/magics_engine_test.py
+-rw-r--r--  2.0 unx     1859 b- defN 23-May-08 23:42 google/generativeai/notebook/model_registry.py
+-rw-r--r--  2.0 unx     1249 b- defN 23-May-08 23:42 google/generativeai/notebook/model_registry_test.py
+-rw-r--r--  2.0 unx     2023 b- defN 23-May-08 23:42 google/generativeai/notebook/output_utils.py
+-rw-r--r--  2.0 unx     3048 b- defN 23-May-08 23:42 google/generativeai/notebook/parsed_args_lib.py
+-rw-r--r--  2.0 unx     5613 b- defN 23-May-08 23:42 google/generativeai/notebook/post_process_utils.py
+-rw-r--r--  2.0 unx     8621 b- defN 23-May-08 23:42 google/generativeai/notebook/post_process_utils_test.py
+-rw-r--r--  2.0 unx      985 b- defN 23-May-08 23:42 google/generativeai/notebook/post_process_utils_test_helper.py
+-rw-r--r--  2.0 unx     2003 b- defN 23-May-08 23:42 google/generativeai/notebook/py_utils.py
+-rw-r--r--  2.0 unx     1717 b- defN 23-May-08 23:42 google/generativeai/notebook/py_utils_test.py
+-rw-r--r--  2.0 unx     2636 b- defN 23-May-08 23:42 google/generativeai/notebook/run_cmd.py
+-rw-r--r--  2.0 unx     2901 b- defN 23-May-08 23:42 google/generativeai/notebook/sheets_id.py
+-rw-r--r--  2.0 unx     2091 b- defN 23-May-08 23:42 google/generativeai/notebook/sheets_id_test.py
+-rw-r--r--  2.0 unx     2933 b- defN 23-May-08 23:42 google/generativeai/notebook/sheets_sanitize_url.py
+-rw-r--r--  2.0 unx     3771 b- defN 23-May-08 23:42 google/generativeai/notebook/sheets_sanitize_url_test.py
+-rw-r--r--  2.0 unx     3808 b- defN 23-May-08 23:42 google/generativeai/notebook/sheets_utils.py
+-rw-r--r--  2.0 unx     2114 b- defN 23-May-08 23:42 google/generativeai/notebook/text_model.py
+-rw-r--r--  2.0 unx     3043 b- defN 23-May-08 23:42 google/generativeai/notebook/text_model_test.py
+-rw-r--r--  2.0 unx      599 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/__init__.py
+-rw-r--r--  2.0 unx    16998 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/llm_function.py
+-rw-r--r--  2.0 unx    14958 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/llm_function_test.py
+-rw-r--r--  2.0 unx     2854 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/llmfn_input_utils.py
+-rw-r--r--  2.0 unx     2312 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/llmfn_inputs_source.py
+-rw-r--r--  2.0 unx     5543 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/llmfn_output_row.py
+-rw-r--r--  2.0 unx     4963 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/llmfn_output_row_test.py
+-rw-r--r--  2.0 unx     7888 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/llmfn_outputs.py
+-rw-r--r--  2.0 unx     5763 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/llmfn_outputs_test.py
+-rw-r--r--  2.0 unx     2468 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/llmfn_post_process.py
+-rw-r--r--  2.0 unx     8058 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/llmfn_post_process_cmds.py
+-rw-r--r--  2.0 unx     6720 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/llmfn_post_process_cmds_test.py
+-rw-r--r--  2.0 unx     1969 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/model.py
+-rw-r--r--  2.0 unx     1232 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/prompt_utils.py
+-rw-r--r--  2.0 unx     1506 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/prompt_utils_test.py
+-rw-r--r--  2.0 unx     1439 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/unique_fn.py
+-rw-r--r--  2.0 unx     1778 b- defN 23-May-08 23:42 google/generativeai/notebook/lib/unique_fn_test.py
+-rw-r--r--  2.0 unx     1018 b- defN 23-May-08 23:42 google/generativeai/types/__init__.py
+-rw-r--r--  2.0 unx     1219 b- defN 23-May-08 23:42 google/generativeai/types/citation_types.py
+-rw-r--r--  2.0 unx     6402 b- defN 23-May-08 23:42 google/generativeai/types/discuss_types.py
+-rw-r--r--  2.0 unx     2894 b- defN 23-May-08 23:42 google/generativeai/types/model_types.py
+-rw-r--r--  2.0 unx     3344 b- defN 23-May-08 23:42 google/generativeai/types/safety_types.py
+-rw-r--r--  2.0 unx     2060 b- defN 23-May-08 23:42 google/generativeai/types/text_types.py
+-rw-r--r--  2.0 unx    11358 b- defN 23-May-08 23:42 google_generativeai-0.1.0rc2.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1328 b- defN 23-May-08 23:42 google_generativeai-0.1.0rc2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-08 23:42 google_generativeai-0.1.0rc2.dist-info/WHEEL
+-rw-r--r--  2.0 unx        7 b- defN 23-May-08 23:42 google_generativeai-0.1.0rc2.dist-info/namespace_packages.txt
+-rw-r--r--  2.0 unx        7 b- defN 23-May-08 23:42 google_generativeai-0.1.0rc2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7738 b- defN 23-May-08 23:42 google_generativeai-0.1.0rc2.dist-info/RECORD
+76 files, 353443 bytes uncompressed, 108733 bytes compressed:  69.2%
```

## zipnote {}

```diff
@@ -1,19 +1,22 @@
-Filename: google_generativeai-0.1.0rc1-py3.10-nspkg.pth
+Filename: google_generativeai-0.1.0rc2-py3.10-nspkg.pth
 Comment: 
 
 Filename: google/generativeai/__init__.py
 Comment: 
 
 Filename: google/generativeai/client.py
 Comment: 
 
 Filename: google/generativeai/discuss.py
 Comment: 
 
+Filename: google/generativeai/docstring_utils.py
+Comment: 
+
 Filename: google/generativeai/models.py
 Comment: 
 
 Filename: google/generativeai/text.py
 Comment: 
 
 Filename: google/generativeai/version.py
@@ -186,35 +189,41 @@
 
 Filename: google/generativeai/notebook/lib/unique_fn_test.py
 Comment: 
 
 Filename: google/generativeai/types/__init__.py
 Comment: 
 
+Filename: google/generativeai/types/citation_types.py
+Comment: 
+
 Filename: google/generativeai/types/discuss_types.py
 Comment: 
 
 Filename: google/generativeai/types/model_types.py
 Comment: 
 
+Filename: google/generativeai/types/safety_types.py
+Comment: 
+
 Filename: google/generativeai/types/text_types.py
 Comment: 
 
-Filename: google_generativeai-0.1.0rc1.dist-info/LICENSE
+Filename: google_generativeai-0.1.0rc2.dist-info/LICENSE
 Comment: 
 
-Filename: google_generativeai-0.1.0rc1.dist-info/METADATA
+Filename: google_generativeai-0.1.0rc2.dist-info/METADATA
 Comment: 
 
-Filename: google_generativeai-0.1.0rc1.dist-info/WHEEL
+Filename: google_generativeai-0.1.0rc2.dist-info/WHEEL
 Comment: 
 
-Filename: google_generativeai-0.1.0rc1.dist-info/namespace_packages.txt
+Filename: google_generativeai-0.1.0rc2.dist-info/namespace_packages.txt
 Comment: 
 
-Filename: google_generativeai-0.1.0rc1.dist-info/top_level.txt
+Filename: google_generativeai-0.1.0rc2.dist-info/top_level.txt
 Comment: 
 
-Filename: google_generativeai-0.1.0rc1.dist-info/RECORD
+Filename: google_generativeai-0.1.0rc2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## google/generativeai/__init__.py

```diff
@@ -12,18 +12,22 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """A high level client library for generative AI.
 
 ## Setup
 
+```posix-terminal
+pip install google-generativeai
+```
+
 ```
 import google.generativeai as genai
 
-genai.configure(api_key=os.environ['API_KEY']
+genai.configure(api_key=os.environ['API_KEY'])
 ```
 
 ## Chat
 
 Use the `genai.chat` function to have a discussion with a model:
 
 ```
@@ -49,25 +53,28 @@
 for model in genai.list_models():
     pprint.pprint(model) # ðŸ¦ŽðŸ¦¦ðŸ¦¬ðŸ¦„
 ```
 
 """
 
 from google.generativeai import types
+from google.generativeai import version
 
 from google.generativeai.discuss import chat
 from google.generativeai.discuss import chat_async
 from google.generativeai.discuss import count_message_tokens
 
 from google.generativeai.text import generate_text
 from google.generativeai.text import generate_embeddings
 
 from google.generativeai.models import list_models
 from google.generativeai.models import get_model
 
-
 from google.generativeai.client import configure
 
+__version__ = version.__version__
+
 del discuss
 del text
 del models
 del client
+del version
```

## google/generativeai/client.py

```diff
@@ -17,16 +17,19 @@
 from typing import cast, Optional, Union
 
 import google.ai.generativelanguage as glm
 
 from google.auth import credentials as ga_credentials
 from google.api_core import client_options as client_options_lib
 from google.api_core import gapic_v1
+from google.generativeai import version
 
 
+USER_AGENT = "genai-py"
+
 default_client_config = {}
 default_discuss_client = None
 default_discuss_async_client = None
 default_model_client = None
 default_text_client = None
 
 
@@ -38,15 +41,15 @@
     # See `_transport_registry` in `DiscussServiceClientMeta`.
     # Since the transport classes align with the client classes it wouldn't make
     # sense to accept a `Transport` object here even though the client classes can.
     # We could accept a dict since all the `Transport` classes take the same args,
     # but that seems rare. Users that need it can just switch to the low level API.
     transport: Union[str, None] = None,
     client_options: Union[client_options_lib.ClientOptions, dict, None] = None,
-    client_info: Optional[gapic_v1.client_info.ClientInfo] = None
+    client_info: Optional[gapic_v1.client_info.ClientInfo] = None,
 ):
     """Captures default client configuration.
 
     If no API key has been provided (either directly, or on `client_options`) and the
     `GOOGLE_API_KEY` environment variable is set, it will be used as the API key.
 
     Args:
@@ -77,14 +80,24 @@
         if api_key is None:
             # If no key is provided explicitly, attempt to load one from the
             # environment.
             api_key = os.getenv("GOOGLE_API_KEY")
 
         client_options.api_key = api_key
 
+    user_agent = f"{USER_AGENT}/{version.__version__}"
+    if client_info:
+        # Be respectful of any existing agent setting.
+        if client_info.user_agent:
+            client_info.user_agent += f" {user_agent}"
+        else:
+            client_info.user_agent = user_agent
+    else:
+        client_info = gapic_v1.client_info.ClientInfo(user_agent=user_agent)
+
     new_default_client_config = {
         "credentials": credentials,
         "transport": transport,
         "client_options": client_options,
         "client_info": client_info,
     }
```

## google/generativeai/discuss.py

```diff
@@ -21,14 +21,15 @@
 
 import google.ai.generativelanguage as glm
 
 from google.generativeai.client import get_default_discuss_client
 from google.generativeai.client import get_default_discuss_async_client
 from google.generativeai.types import discuss_types
 from google.generativeai.types import model_types
+from google.generativeai.types import safety_types
 
 
 def _make_message(content: discuss_types.MessageOptions) -> glm.Message:
     if isinstance(content, glm.Message):
         return content
     if isinstance(content, str):
         return glm.Message(content=content)
@@ -311,15 +312,15 @@
            top-k sampling.
 
            `top_p` configures the nucleus sampling. It sets the maximum cumulative
             probability of tokens to sample from.
 
             For example, if the sorted probabilities are
             `[0.5, 0.2, 0.1, 0.1, 0.05, 0.05]` a `top_p` of `0.8` will sample
-            as `[0.625, 0.25, 0.125, 0, 0, 0].
+            as `[0.625, 0.25, 0.125, 0, 0, 0]`.
 
             Typical values are in the `[0.9, 1.0]` range.
         prompt: You may pass a `types.MessagePromptOptions` **instead** of a
             setting `context`/`examples`/`messages`, but not both.
         client: If you're not relying on the default client, you pass a
             `glm.DiscussServiceClient` instead.
 
@@ -385,32 +386,43 @@
 
     def __init__(self, **kwargs):
         for key, value in kwargs.items():
             setattr(self, key, value)
 
     @property
     @set_doc(discuss_types.ChatResponse.last.__doc__)
-    def last(self) -> str:
-        return self.messages[-1]["content"]
+    def last(self) -> Optional[str]:
+        if self.messages[-1]:
+            return self.messages[-1]["content"]
+        else:
+            return None
 
     @last.setter
     def last(self, message: discuss_types.MessageOptions):
         message = _make_message(message)
         self.messages[-1] = message
 
     @set_doc(discuss_types.ChatResponse.reply.__doc__)
     def reply(
         self, message: discuss_types.MessageOptions
     ) -> discuss_types.ChatResponse:
         if isinstance(self._client, glm.DiscussServiceAsyncClient):
             raise TypeError(
                 f"reply can't be called on an async client, use reply_async instead."
             )
+        if self.last is None:
+            raise ValueError(
+                "The last response from the model did not return any candidates.\n"
+                "Check the `.filters` attribute to see why the responses were filtered:\n"
+                f"{self.filters}"
+            )
+
         request = self.to_dict()
         request.pop("candidates")
+        request.pop("filters", None)
         request["messages"] = list(request["messages"])
         request["messages"].append(_make_message(message))
         request = _make_generate_message_request(**request)
         return _generate_response(request=request, client=self._client)
 
     @set_doc(discuss_types.ChatResponse.reply.__doc__)
     async def reply_async(
@@ -418,14 +430,15 @@
     ) -> discuss_types.ChatResponse:
         if isinstance(self._client, glm.DiscussServiceClient):
             raise TypeError(
                 f"reply_async can't be called on a non-async client, use reply instead."
             )
         request = self.to_dict()
         request.pop("candidates")
+        request.pop("filters")
         request["messages"] = list(request["messages"])
         request["messages"].append(_make_message(message))
         request = _make_generate_message_request(**request)
         return await _generate_response_async(request=request, client=self._client)
 
 
 def _build_chat_response(
@@ -436,20 +449,28 @@
     request = type(request).to_dict(request)
     prompt = request.pop("prompt")
     request["examples"] = prompt["examples"]
     request["context"] = prompt["context"]
     request["messages"] = prompt["messages"]
 
     response = type(response).to_dict(response)
-    request["messages"].append(response["candidates"][0])
+    response.pop("messages")
+
+    response["filters"] = safety_types.convert_filters_to_enums(response["filters"])
+
+    if response["candidates"]:
+        last = response["candidates"][0]
+    else:
+        last = None
+    request["messages"].append(last)
     request.setdefault("temperature", None)
     request.setdefault("candidate_count", None)
 
     return ChatResponse(
-        _client=client, candidates=response["candidates"], **request
+        _client=client, **response, **request
     )  # pytype: disable=missing-parameter
 
 
 def _generate_response(
     request: glm.GenerateMessageRequest,
     client: Optional[glm.DiscussServiceClient] = None,
 ) -> ChatResponse:
```

## google/generativeai/text.py

```diff
@@ -20,14 +20,15 @@
 from typing import List, Iterable, Iterator, Optional, Union
 
 import google.ai.generativelanguage as glm
 
 from google.generativeai.client import get_default_text_client
 from google.generativeai.types import text_types
 from google.generativeai.types import model_types
+from google.generativeai.types import safety_types
 
 
 def _make_text_prompt(prompt: Union[str, dict[str, str]]) -> glm.TextPrompt:
     if isinstance(prompt, str):
         return glm.TextPrompt(text=prompt)
     elif isinstance(prompt, dict):
         return glm.TextPrompt(prompt)
@@ -40,14 +41,15 @@
     model: model_types.ModelNameOptions = "models/chat-lamda-001",
     prompt: Optional[str] = None,
     temperature: Optional[float] = None,
     candidate_count: Optional[int] = None,
     max_output_tokens: Optional[int] = None,
     top_p: Optional[int] = None,
     top_k: Optional[int] = None,
+    safety_settings: Optional[List[safety_types.SafetySettingDict]] = None,
     stop_sequences: Union[str, Iterable[str]] = None,
 ) -> glm.GenerateTextRequest:
     model = model_types.make_model_name(model)
     prompt = _make_text_prompt(prompt=prompt)
     if isinstance(stop_sequences, str):
         stop_sequences = [stop_sequences]
     if stop_sequences:
@@ -57,27 +59,29 @@
         model=model,
         prompt=prompt,
         temperature=temperature,
         candidate_count=candidate_count,
         max_output_tokens=max_output_tokens,
         top_p=top_p,
         top_k=top_k,
+        safety_settings=safety_settings,
         stop_sequences=stop_sequences,
     )
 
 
 def generate_text(
     *,
     model: Optional[model_types.ModelNameOptions] = "models/text-bison-001",
     prompt: str,
     temperature: Optional[float] = None,
     candidate_count: Optional[int] = None,
     max_output_tokens: Optional[int] = None,
     top_p: Optional[float] = None,
     top_k: Optional[float] = None,
+    safety_settings: Optional[Iterable[safety.SafetySettingDict]] = None,
     stop_sequences: Union[str, Iterable[str]] = None,
     client: Optional[glm.TextServiceClient] = None,
 ) -> text_types.Completion:
     """Calls the API and returns a `types.Completion` containing the response.
 
     Args:
         model: Which model to call, as a string or a `types.Model`.
@@ -99,14 +103,23 @@
             `top_k` sets the maximum number of tokens to sample from on each step.
         top_p: The API uses combined [nucleus](https://arxiv.org/abs/1904.09751) and top-k sampling.
             `top_p` configures the nucleus sampling. It sets the maximum cumulative
             probability of tokens to sample from.
             For example, if the sorted probabilities are
             `[0.5, 0.2, 0.1, 0.1, 0.05, 0.05]` a `top_p` of `0.8` will sample
             as `[0.625, 0.25, 0.125, 0, 0, 0].
+        safety_settings: A list of unique `types.SafetySetting` instances for blocking unsafe content.
+           These will be enforced on the `prompt` and
+           `candidates`. There should not be more than one
+           setting for each `types.SafetyCategory` type. The API will block any prompts and
+           responses that fail to meet the thresholds set by these settings. This list
+           overrides the default settings for each `SafetyCategory` specified in the
+           safety_settings. If there is no `types.SafetySetting` for a given
+           `SafetyCategory` provided in the list, the API will use the default safety
+           setting for that category.
         stop_sequences: A set of up to 5 character sequences that will stop output generation.
           If specified, the API will stop at the first appearance of a stop
           sequence. The stop sequence will not be included as part of the response.
         client: If you're not relying on a default client, you pass a `glm.TextServiceClient` instead.
 
     Returns:
         A `types.Completion` containing the model's text completion response.
@@ -115,14 +128,15 @@
         model=model,
         prompt=prompt,
         temperature=temperature,
         candidate_count=candidate_count,
         max_output_tokens=max_output_tokens,
         top_p=top_p,
         top_k=top_k,
+        safety_settings=safety_settings,
         stop_sequences=stop_sequences,
     )
 
     return _generate_response(client=client, request=request)
 
 
 @dataclasses.dataclass(init=False)
@@ -141,14 +155,22 @@
 ) -> Completion:
     if client is None:
         client = get_default_text_client()
 
     response = client.generate_text(request)
     response = type(response).to_dict(response)
 
+    response["filters"] = safety_types.convert_filters_to_enums(response["filters"])
+    response["safety_feedback"] = safety_types.convert_safety_feedback_to_enums(
+        response["safety_feedback"]
+    )
+    response["candidates"] = safety_types.convert_candidate_enums(
+        response["candidates"]
+    )
+
     return Completion(_client=client, **response)
 
 
 def generate_embeddings(model: str, text: str, client: glm.TextServiceClient = None):
     """Calls the API to create an embedding for the text passed in.
 
     Args:
```

## google/generativeai/version.py

```diff
@@ -9,8 +9,8 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = "0.1.0rc1"
+__version__ = "0.1.0rc2"
```

## google/generativeai/notebook/magics.py

```diff
@@ -17,25 +17,31 @@
 Installs %%palm magics.
 """
 from __future__ import annotations
 
 import abc
 
 from google.auth import credentials
+from google.generativeai import client as palm
 from google.generativeai.notebook import gspread_client
 from google.generativeai.notebook import ipython_env
 from google.generativeai.notebook import ipython_env_impl
 from google.generativeai.notebook import magics_engine
 from google.generativeai.notebook import post_process_utils
 from google.generativeai.notebook import sheets_utils
 
 import IPython
 from IPython.core import magic
 
 
+# Set the UA to distinguish the magic from the client. Do this at import-time
+# so that a user can still call `palm.configure()`, and both their settings
+# and this are honored.
+palm.USER_AGENT = "genai-py-magic"
+
 SheetsInputs = sheets_utils.SheetsInputs
 SheetsOutputs = sheets_utils.SheetsOutputs
 
 # Decorator functions for post-processing.
 post_process_add_fn = post_process_utils.post_process_add_fn
 post_process_replace_fn = post_process_utils.post_process_replace_fn
```

## google/generativeai/types/__init__.py

```diff
@@ -12,10 +12,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """A collection of type definitions used throughout the library."""
 
 from google.generativeai.types.discuss_types import *
 from google.generativeai.types.model_types import *
+from google.generativeai.types.text_types import *
+from google.generativeai.types.citation_types import *
+from google.generativeai.types.safety_types import *
 
 del discuss_types
 del model_types
+del text_types
+del citation_types
+del safety_types
```

## google/generativeai/types/discuss_types.py

```diff
@@ -15,14 +15,16 @@
 """Type definitions for the discuss service."""
 
 import abc
 import dataclasses
 from typing import Any, Dict, TypedDict, Union, Iterable, Optional, Tuple, List
 
 import google.ai.generativelanguage as glm
+from google.generativeai.types import safety_types
+from google.generativeai.types import citation_types
 
 __all__ = [
     "MessageDict",
     "MessageOptions",
     "MessagesOptions",
     "ExampleDict",
     "ExampleOptions",
@@ -31,19 +33,20 @@
     "MessagePromptOptions",
     "ResponseDict",
     "ChatResponse",
     "AuthorError",
 ]
 
 
-class MessageDict(TypedDict, total=False):
+class MessageDict(TypedDict):
     """A dict representation of a `glm.Message`."""
 
     author: str
     content: str
+    citation_metadata: Optional[citation_types.CitationMetadataDict]
 
 
 MessageOptions = Union[str, MessageDict, glm.Message]
 MESSAGE_OPTIONS = (str, dict, glm.Message)
 
 MessagesOptions = Union[
     MessageOptions,
@@ -125,40 +128,49 @@
 
             This list will contain a *maximum* of `candidate_count` candidates.
             It may contain fewer (duplicates are dropped), it will contain at least one.
 
             Note: The `temperature` field affects the variability of the responses. Low
             temperatures will return few candidates. Setting `temperature=0` is deterministic,
             so it will only ever return one candidate.
+        filters: This indicates which `types.SafetyCategory`(s) blocked a
+           candidate from this response, the lowest `types.HarmProbability`
+           that triggered a block, and the `types.HarmThreshold` setting for that category.
+           This indicates the smallest change to the `types.SafetySettings` that would be
+           necessary to unblock at least 1 response.
 
+           The blocking is configured by the `types.SafetySettings` in the request (or the
+           default `types.SafetySettings` of the API).
         messages: Contains all the `messages` that were passed when the model was called,
             plus the top `candidate` message.
         model: The model name.
         context: Text that should be provided to the model first, to ground the response.
         examples: Examples of what the model should generate.
         messages: A snapshot of the conversation history sorted chronologically.
         temperature: Controls the randomness of the output. Must be positive.
         candidate_count: The **maximum** number of generated response messages to return.
         top_k: The maximum number of tokens to consider when sampling.
         top_p: The maximum cumulative probability of tokens to consider when sampling.
+
     """
 
     model: str
     context: str
     examples: List[ExampleDict]
-    messages: List[MessageDict]
+    messages: List[Optional[MessageDict]]
     temperature: Optional[float]
     candidate_count: Optional[int]
     candidates: List[MessageDict]
     top_p: Optional[float] = None
     top_k: Optional[float] = None
+    filters: List[safety_types.ContentFilterDict]
 
     @property
     @abc.abstractmethod
-    def last(self) -> str:
+    def last(self) -> Optional[str]:
         """A settable property that provides simple access to the last response string
 
         A shortcut for `response.messages[0]['content']`.
         """
         pass
 
     def to_dict(self) -> Dict[str, Any]:
```

## google/generativeai/types/text_types.py

```diff
@@ -13,32 +13,46 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import abc
 import dataclasses
 from typing import Any, Dict, Optional, List, Iterator, TypedDict
 
+from google.generativeai.types import safety_types
+from google.generativeai.types import citation_types
+
+
 __all__ = ["Completion"]
 
 
-class TextCandidate(TypedDict, total=False):
+class TextCompletion(TypedDict, total=False):
     output: str
+    safety_ratings: Optional[List[safety_types.SafetyRatingDict]]
+    citation_metadata: Optional[citation_types.CitationMetadataDict]
 
 
 @dataclasses.dataclass(init=False)
 class Completion(abc.ABC):
-    """A text completion given a prompt from the model.
+    """The result returned by `generativeai.generate_text`.
 
-    * Use `completion.candidates` to access all of the text completion options generated by the model.
+    Use `GenerateTextResponse.candidates` to access all the completions generated by the model.
 
     Attributes:
         candidates: A list of candidate text completions generated by the model.
+        result: The output of the first candidate,
+        filters: Indicates the reasons why content may have been blocked
+          Either Unspecified, Safety, or Other. See `types.ContentFilter`.
+        safety_feedback: Indicates which safety settings blocked content in this result.
     """
 
-    candidates: List[TextCandidate]
+    candidates: List[TextCompletion]
     result: Optional[str]
+    filters: Optional[list[safety_types.ContentFilterDict]]
+    safety_feedback: Optional[list[safety_types.SafetyFeedbackDict]]
 
     def to_dict(self) -> Dict[str, Any]:
         result = {
             "candidates": self.candidates,
+            "filters": self.filters,
+            "safety_feedback": self.safety_feedback,
         }
         return result
```

## Comparing `google_generativeai-0.1.0rc1-py3.10-nspkg.pth` & `google_generativeai-0.1.0rc2-py3.10-nspkg.pth`

 * *Files identical despite different names*

## Comparing `google_generativeai-0.1.0rc1.dist-info/LICENSE` & `google_generativeai-0.1.0rc2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `google_generativeai-0.1.0rc1.dist-info/METADATA` & `google_generativeai-0.1.0rc2.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: google-generativeai
-Version: 0.1.0rc1
+Version: 0.1.0rc2
 Summary: Google Generative AI High level API client library and tools.
 Home-page: https://github.com/google/generative-ai-python
 Author: Google LLC
 Author-email: googleapis-packages@google.com
 License: Apache 2.0
 Platform: Posix; MacOS X; Windows
 Classifier: Development Status :: 4 - Beta
@@ -16,17 +16,18 @@
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: Operating System :: OS Independent
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Requires-Python: >=3.8
 License-File: LICENSE
-Requires-Dist: google-ai-generativelanguage (==0.1.0)
+Requires-Dist: google-ai-generativelanguage (==0.2.0)
 Provides-Extra: dev
 Requires-Dist: absl-py ; extra == 'dev'
 Requires-Dist: asynctest ; extra == 'dev'
 Requires-Dist: black ; extra == 'dev'
 Requires-Dist: nose2 ; extra == 'dev'
+Requires-Dist: pandas ; extra == 'dev'
 Requires-Dist: pytype ; extra == 'dev'
 Requires-Dist: pyyaml ; extra == 'dev'
 
 This is a repo for the generative language client library.
```

## Comparing `google_generativeai-0.1.0rc1.dist-info/RECORD` & `google_generativeai-0.1.0rc2.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,15 @@
-google_generativeai-0.1.0rc1-py3.10-nspkg.pth,sha256=xH5gTxc4UipYP3qrbP-4CCHNGBV97eBR4QqhheCvBl4,539
-google/generativeai/__init__.py,sha256=KrMOet2B2Elxn-voF8iUeSOb9quhXqZVXpmdaXVHYxY,1835
-google/generativeai/client.py,sha256=r2C1nrRu5JAlOKPjxNZisJorOXEKwEgZ52q9ILOucOE,5241
-google/generativeai/discuss.py,sha256=VNnYzrFqqNgapiSRWxFSi_G-sxfuuRBCs52MQavio38,16965
+google_generativeai-0.1.0rc2-py3.10-nspkg.pth,sha256=xH5gTxc4UipYP3qrbP-4CCHNGBV97eBR4QqhheCvBl4,539
+google/generativeai/__init__.py,sha256=QbV7psH_UKi6L_j__wuf4rtQ0IzgCTtnbV-NSuxnVik,1977
+google/generativeai/client.py,sha256=x-tHInRfEvbu3AbKs8o-mI_PqsNQsAEPDJ1YKiJsiHA,5677
+google/generativeai/discuss.py,sha256=xztYm-olK4846uFQLjTkDRHan2tfojBrZez63J-ky_8,17625
+google/generativeai/docstring_utils.py,sha256=ylA-ZyRfLbj8lfRXuAXlIMHcL_ohaGiA3RIoMsldk6A,845
 google/generativeai/models.py,sha256=2AhmYPiGOtELaHUAE2-ipj2qByn_sfeP0lNY43Auuh4,2946
-google/generativeai/text.py,sha256=ATJy_LnrHGeFBubmEYwic3A45K3YMhqGU-4_RmhEdHs,6932
-google/generativeai/version.py,sha256=4AtZdDbB315UQLMc4ppkrxCeabVv2FnTFL1F4PNMbQc,624
+google/generativeai/text.py,sha256=0dJmvxf_ERpOENvG75xlPYjrFQnQPQxotnj-WXBffJU,8209
+google/generativeai/version.py,sha256=nPu4k39GROiZL07B_ph-cxvwgdiu6WJ9L8wy4QXONIQ,624
 google/generativeai/notebook/__init__.py,sha256=ru4_SIILC3oVxMp14hFHkhZCim_XO6Wr61AS7vhEjKg,1123
 google/generativeai/notebook/argument_parser.py,sha256=CPpIcEzXNJIs0sDLwGMjQePAlD9KdISwi3LDqXaM4WI,3791
 google/generativeai/notebook/argument_parser_test.py,sha256=nhdccW5DPuFxf0FL5KATI3K8eZCTQnyGNYPGH0QEhhE,1910
 google/generativeai/notebook/cmd_line_parser.py,sha256=Yvf5dmAASgxhQ_1ECYu78Ct1wcbiT6UpSLgv6PUkN1U,19612
 google/generativeai/notebook/cmd_line_parser_test.py,sha256=lAf16-UIDIl1hmOMtDtZpJ5ZUIamhvDhSPPeyCvSaQE,15213
 google/generativeai/notebook/command.py,sha256=SXMqxE9GUWrbbdRkZx90oq_AEzBwp_lhR9e1DBaAPoI,1503
 google/generativeai/notebook/command_utils.py,sha256=XDL6V9cBm4ZIUzrNRFdYu1C_xjklyLRbGWVz18SBUF4,6049
@@ -20,15 +21,15 @@
 google/generativeai/notebook/gspread_client.py,sha256=U7V0QChdfJx6SewWOV9GULkbEUcSVeakCldiirbQrVY,7450
 google/generativeai/notebook/html_utils.py,sha256=10KElAnilYaMU1_skctgeOc9kGnob4RGwimx3vYeiJE,1513
 google/generativeai/notebook/html_utils_test.py,sha256=x7rNvU9AdLUiRQ-eH6vGvSnuYe2W7jnY1X0D_Kw9K-0,1802
 google/generativeai/notebook/input_utils.py,sha256=A5-ue1R67LeH0tvBKlbMITYI1IQDUu8UMO8udZAQgNI,2743
 google/generativeai/notebook/input_utils_test.py,sha256=mXkNBXsMazxiwEds8D-ADnxhAhC2Pi7I4ibt99aAqAM,2396
 google/generativeai/notebook/ipython_env.py,sha256=Kn1BgJ9VuHW4RrcmcO54EE3qGHFCOCJbXZO8H5d6CDU,1636
 google/generativeai/notebook/ipython_env_impl.py,sha256=WuAvchEKhia6T2p9eaOhKoK1PRFKlCjJrGwOSSHTp_A,1044
-google/generativeai/notebook/magics.py,sha256=Qh89tHRuZ8I_orH8J9I7vMmPy36wMGqZ2maram2Sr20,3895
+google/generativeai/notebook/magics.py,sha256=ygzhQDkr0U-LSrVaKTTJXN2sMq15r2UydYviL3qLEig,4156
 google/generativeai/notebook/magics_engine.py,sha256=cWtEtz2LYJ34-o10RcbjsqFFgN6HN864uWl2VszSIrk,5294
 google/generativeai/notebook/magics_engine_test.py,sha256=JrYPk_d-Zxzuobv9JITXhk7WgFwss8laPyzlenxlVlg,24832
 google/generativeai/notebook/model_registry.py,sha256=ORNRhvy-nE3kS6MtkJgUbw0ZmrzLAi8w_iHPXylJRM8,1859
 google/generativeai/notebook/model_registry_test.py,sha256=lVNTeOP8c7K-SaXNK0FpsFhO65sbIZk93ttqnMT1FAk,1249
 google/generativeai/notebook/output_utils.py,sha256=nIS6zxQXr9s_qdmzjvGXCpcaSZ5CQmTSriScnYTv2Tc,2023
 google/generativeai/notebook/parsed_args_lib.py,sha256=GdaGAFghqCK5ZEDiEwQsn1-uLR1OPyZJPr9XEhQKg70,3048
 google/generativeai/notebook/post_process_utils.py,sha256=lgek8a8V5veL_cRIfi6XZFeKpthTVtTIpyo0TwMQGmU,5613
@@ -57,17 +58,19 @@
 google/generativeai/notebook/lib/llmfn_post_process_cmds.py,sha256=YLCK2M1ljujtS2AaJzvVlims9ye2zpFKjx4UIHW43j8,8058
 google/generativeai/notebook/lib/llmfn_post_process_cmds_test.py,sha256=92fNZHSJWLVRUXZBwO-t9bkGzYZpeWkeYUKlaPxELpQ,6720
 google/generativeai/notebook/lib/model.py,sha256=rc_ngm_MnEe7J5d9qvZe6oP1jIj_m38gq7xT0KUfRJ4,1969
 google/generativeai/notebook/lib/prompt_utils.py,sha256=870ygPJ-DJgm9qpecGyPnsDmIpzgWcVsUXGG5L34kW8,1232
 google/generativeai/notebook/lib/prompt_utils_test.py,sha256=qUxZmgJRdjf7g5toXLjwk4mMi-sgz31f5oHAnkp_DEY,1506
 google/generativeai/notebook/lib/unique_fn.py,sha256=QoElQhsLvAZK5_h6W2bBN6rsduc5WOELihOVzTfDGOs,1439
 google/generativeai/notebook/lib/unique_fn_test.py,sha256=U48JTYyL1lbwkqooxtJGuw6dfjFb4-dz7rD_40g8lBE,1778
-google/generativeai/types/__init__.py,sha256=k7XSJTOiwGZs1ygjttevzKJy2efah5dJgWqWSESC12o,808
-google/generativeai/types/discuss_types.py,sha256=8xC6UJWCC7UxFcnH9T3KmOLmmq0O9oNLIwdWXiosfZc,5640
+google/generativeai/types/__init__.py,sha256=dKcR5ScB66Ir7wAHbcq6NZvlP9p_HLd6US2atBwKXs0,1018
+google/generativeai/types/citation_types.py,sha256=lIF01QijZnfZ6Ik4VFxOI5cP2UObdEfpv6t9QujLg-A,1219
+google/generativeai/types/discuss_types.py,sha256=8L01JBp_wp3yRhigs94LDLbDFkEg7WpQrkqqyb8DbeQ,6402
 google/generativeai/types/model_types.py,sha256=8FEoZMxRrnMau0-5r10pnPKU9lnFd7Rxrg-LC_Xj8wQ,2894
-google/generativeai/types/text_types.py,sha256=ZTWu_U0ThOpohQNDZjn6i0mO3NvauIJLx-hzqab5rdg,1313
-google_generativeai-0.1.0rc1.dist-info/LICENSE,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
-google_generativeai-0.1.0rc1.dist-info/METADATA,sha256=t6dQwO8koHcG5udiJb2Uf_iw3cMvOMOuftYIzn9tE8s,1289
-google_generativeai-0.1.0rc1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-google_generativeai-0.1.0rc1.dist-info/namespace_packages.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
-google_generativeai-0.1.0rc1.dist-info/top_level.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
-google_generativeai-0.1.0rc1.dist-info/RECORD,,
+google/generativeai/types/safety_types.py,sha256=SNwYYUzVo9bYowzhd13LQVve29pjtHwBkCQrC18B8lU,3344
+google/generativeai/types/text_types.py,sha256=AuT_x1s7xLkzYzkjB0P0_1tbwGdvtAgVrfc42rFbFAc,2060
+google_generativeai-0.1.0rc2.dist-info/LICENSE,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
+google_generativeai-0.1.0rc2.dist-info/METADATA,sha256=Iuat63xyacUH3yeT8bi-slzKn2P0DSvJ8NtAa1xZfBI,1328
+google_generativeai-0.1.0rc2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+google_generativeai-0.1.0rc2.dist-info/namespace_packages.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
+google_generativeai-0.1.0rc2.dist-info/top_level.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
+google_generativeai-0.1.0rc2.dist-info/RECORD,,
```

